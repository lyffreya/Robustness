{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Blur "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import os.path\n",
    "import time\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as trn\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "from PIL import Image\n",
    "\n",
    "import skimage as sk\n",
    "from skimage.filters import gaussian\n",
    "from io import BytesIO\n",
    "from wand.image import Image as WandImage\n",
    "from wand.api import library as wandlibrary\n",
    "import wand.color as WandColor\n",
    "import ctypes\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "from scipy.ndimage import zoom as scizoom\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ImageNet data\n",
      "contrast 1\n",
      "contrast 2\n",
      "contrast 3\n",
      "contrast 4\n",
      "contrast 5\n"
     ]
    }
   ],
   "source": [
    "# /////////////// Data Loader ///////////////\n",
    "\n",
    "\n",
    "IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm']\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    \"\"\"Checks if a file is an image.\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for target in sorted(os.listdir(dir)):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "class DistortImageFolder(data.Dataset):\n",
    "    def __init__(self, root, method, severity, transform=None, target_transform=None,\n",
    "                 loader=default_loader):\n",
    "        classes, class_to_idx = find_classes(root)\n",
    "        imgs = make_dataset(root, class_to_idx)\n",
    "        if len(imgs) == 0:\n",
    "            raise (RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                                                                             \"Supported image extensions are: \" + \",\".join(\n",
    "                IMG_EXTENSIONS)))\n",
    "\n",
    "        self.root = root\n",
    "        self.method = method\n",
    "        self.severity = severity\n",
    "        self.imgs = imgs\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            img = self.method(img, self.severity)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        save_path = '/home/jtang/Desktop/DistortedImageNet/JPEG/' + self.method.__name__ + \\\n",
    "                    '/' + str(self.severity) + '/' + self.idx_to_class[target]\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        save_path += path[path.rindex('/'):]\n",
    "\n",
    "        Image.fromarray(np.uint8(img)).save(save_path, quality=85, optimize=True)\n",
    "\n",
    "        return 0  # we do not care about returning the data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "# /////////////// Distortion Helpers ///////////////\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tell Python about the C method\n",
    "wandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n",
    "                                              ctypes.c_double,  # radius\n",
    "                                              ctypes.c_double,  # sigma\n",
    "                                              ctypes.c_double)  # angle\n",
    "\n",
    "\n",
    "# Extend wand.image.Image class to include method signature\n",
    "class MotionImage(WandImage):\n",
    "    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n",
    "        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n",
    "\n",
    "\n",
    "\n",
    "# /////////////// End Distortion Helpers ///////////////\n",
    "\n",
    "\n",
    "# /////////////// Distortions ///////////////\n",
    "\n",
    "def gaussian_noise(x, severity=1):\n",
    "    c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n",
    "\n",
    "'''\n",
    "    c = [0.4, .3, .2, .1, .05][severity - 1]\n",
    "    # c = 2.0\n",
    "    x = np.array(x) / 255.\n",
    "    means = np.mean(x, axis=(0, 1), keepdims=True)\n",
    "    return np.clip((x - means) * c + means, 0, 1) * 255.\n",
    "'''\n",
    "\n",
    "def contrast(x, severity=1):\n",
    "    c = [1.1, 1.2, 1.3, 1.4, 1.5] [severity-1]\n",
    "    x = np.array(x) / 255.\n",
    "    return np.clip((x - 128.0/255.0) * c + 128.0/255.0, 0, 1) * 255.\n",
    "\n",
    "\n",
    "def brightness(x, severity=1):\n",
    "    c = [.1, .2, .3, .4, .5][severity - 1]\n",
    "    x = np.array(x) / 255.\n",
    "    x = sk.color.rgb2hsv(x)\n",
    "    x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n",
    "    x = sk.color.hsv2rgb(x)\n",
    "    return np.clip(x, 0, 1) * 255\n",
    "\n",
    "\n",
    "def saturate(x, severity=1):\n",
    "    c = [(0.1, 0), (0.2, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    x = sk.color.rgb2hsv(x)\n",
    "    x[:, :, 1] = np.clip(x[:, :, 1] * c[0] + c[1], 0, 1)\n",
    "    x = sk.color.hsv2rgb(x)\n",
    "\n",
    "    return np.clip(x, 0, 1) * 255\n",
    "\n",
    "# Mix up Augmentation\n",
    "def color_space(x, severity = 1):\n",
    "    if severity == 0:\n",
    "        severity = randrange(5)+1\n",
    "    \n",
    "    aug_func = randrange(4)\n",
    "    # transfer to np array\n",
    "    x = np.array(x) / 255.\n",
    "    \n",
    "    if aug_func == 0:\n",
    "        # contrast\n",
    "        contrast_c = [1.1, 1.2, 1.3, 1.4, 1.5] [severity-1]\n",
    "        x = np.clip((x - 128.0/255.0) * contrast_c + 128.0/255.0, 0, 1)\n",
    "    elif aug_func == 1:\n",
    "        # brightness \n",
    "        brightness_c = [.1, .2, .3, .4, .5][severity - 1]\n",
    "        # RGB to HSV (Hue, Saturation, Value) conversion\n",
    "        x_hsv = sk.color.rgb2hsv(x)\n",
    "        x_hsv[:, :, 2] = np.clip(x_hsv[:, :, 2] + brightness_c, 0, 1)\n",
    "        x = sk.color.hsv2rgb(x_hsv)\n",
    "    elif aug_func ==2:\n",
    "        # saturation\n",
    "        saturate_c = [(0.1, 0), (0.3, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n",
    "        x_hsv = sk.color.rgb2hsv(x)\n",
    "        x_hsv[:, :, 1] = np.clip(x_hsv[:, :, 1] * saturate_c[0] + saturate_c[1], 0, 1)\n",
    "        x = sk.color.hsv2rgb(x_hsv)\n",
    "    elif aug_func ==3:\n",
    "        # contrast\n",
    "        contrast_c = [1.1, 1.2, 1.3, 1.4, 1.5] [severity-1]\n",
    "        x = np.clip((x - 128.0/255.0) * contrast_c + 128.0/255.0, 0, 1)\n",
    "        # brightness & saturation\n",
    "        brightness_c = [.1, .2, .3, .4, .5][severity - 1]\n",
    "        saturate_c = [(0.1, 0), (0.3, 0), (2, 0), (5, 0.1), (20, 0.2)][severity - 1]\n",
    "        # RGB to HSV (Hue, Saturation, Value) conversion\n",
    "        x_hsv = sk.color.rgb2hsv(x)\n",
    "        x_hsv[:, :, 2] = np.clip(x_hsv[:, :, 2] + brightness_c, 0, 1)\n",
    "        x_hsv[:, :, 1] = np.clip(x_hsv[:, :, 1] * saturate_c[0] + saturate_c[1], 0, 1)\n",
    "        x = sk.color.hsv2rgb(x_hsv)\n",
    "    x = np.clip(x, 0, 1) * 255.\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "# /////////////// Further Setup ///////////////\n",
    "\n",
    "\n",
    "def save_distorted(method=gaussian_noise):\n",
    "    for severity in range(1, 6):\n",
    "        #severity = 0\n",
    "        print(method.__name__, severity)\n",
    "        distorted_dataset = DistortImageFolder(\n",
    "            root=\"/home/jtang/Desktop/val\",\n",
    "            method=method, severity=severity,\n",
    "            transform=trn.Compose([trn.Resize(256), trn.CenterCrop(224)]))\n",
    "        distorted_dataset_loader = torch.utils.data.DataLoader(\n",
    "            distorted_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "    for _ in distorted_dataset_loader: continue\n",
    "\n",
    "\n",
    "# /////////////// Display Results ///////////////\n",
    "import collections\n",
    "\n",
    "print('\\nUsing ImageNet data')\n",
    "\n",
    "d = collections.OrderedDict()\n",
    "\n",
    "\n",
    "d['Brightness'] = brightness\n",
    "d['Contrast'] = contrast\n",
    "d['Saturate'] = saturate\n",
    "\n",
    "\n",
    "\n",
    "# or method_name in d.keys():\n",
    "#    save_distorted(d[method_name])\n",
    "\n",
    "#save_distorted(color_space)\n",
    "save_distorted(contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ImageNet data\n",
      "contrast 3\n"
     ]
    }
   ],
   "source": [
    "# /////////////// Data Loader ///////////////\n",
    "\n",
    "\n",
    "IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm']\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    \"\"\"Checks if a file is an image.\n",
    "    Args:\n",
    "        filename (string): path to a file\n",
    "    Returns:\n",
    "        bool: True if the filename ends with a known image extension\n",
    "    \"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)\n",
    "\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "    images = []\n",
    "    dir = os.path.expanduser(dir)\n",
    "    for target in sorted(os.listdir(dir)):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in sorted(fnames):\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "    return images\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n",
    "\n",
    "\n",
    "class DistortImageFolder(data.Dataset):\n",
    "    def __init__(self, root, method, severity, transform=None, target_transform=None,\n",
    "                 loader=default_loader):\n",
    "        classes, class_to_idx = find_classes(root)\n",
    "        imgs = make_dataset(root, class_to_idx)\n",
    "        if len(imgs) == 0:\n",
    "            raise (RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                                                                             \"Supported image extensions are: \" + \",\".join(\n",
    "                IMG_EXTENSIONS)))\n",
    "\n",
    "        self.root = root\n",
    "        self.method = method\n",
    "        self.severity = severity\n",
    "        self.imgs = imgs\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            img = self.method(img, self.severity)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        # /data/imagenet_augment_CAE/\n",
    "        # /home/jtang/Desktop/DistortedImageNet/JPEG/\n",
    "        save_path = '/data/imagenet_augment_contrast/' + self.method.__name__ + \\\n",
    "                    '/' + str(self.severity) + '/' + self.idx_to_class[target]\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        save_path += path[path.rindex('/'):]\n",
    "\n",
    "        Image.fromarray(np.uint8(img)).save(save_path, quality=85, optimize=True)\n",
    "\n",
    "        return 0  # we do not care about returning the data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "# /////////////// Distortion Helpers ///////////////\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tell Python about the C method\n",
    "wandlibrary.MagickMotionBlurImage.argtypes = (ctypes.c_void_p,  # wand\n",
    "                                              ctypes.c_double,  # radius\n",
    "                                              ctypes.c_double,  # sigma\n",
    "                                              ctypes.c_double)  # angle\n",
    "\n",
    "\n",
    "# Extend wand.image.Image class to include method signature\n",
    "class MotionImage(WandImage):\n",
    "    def motion_blur(self, radius=0.0, sigma=0.0, angle=0.0):\n",
    "        wandlibrary.MagickMotionBlurImage(self.wand, radius, sigma, angle)\n",
    "\n",
    "\n",
    "\n",
    "# /////////////// Distortions ///////////////\n",
    "\n",
    "def gaussian_noise(x, severity=1):\n",
    "    c = [.08, .12, 0.18, 0.26, 0.38][severity - 1]\n",
    "\n",
    "    x = np.array(x) / 255.\n",
    "    return np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255\n",
    "\n",
    "'''\n",
    "    c = [0.4, .3, .2, .1, .05][severity - 1]\n",
    "    # c = 2.0\n",
    "    x = np.array(x) / 255.\n",
    "    means = np.mean(x, axis=(0, 1), keepdims=True)\n",
    "    return np.clip((x - means) * c + means, 0, 1) * 255.\n",
    "'''\n",
    "\n",
    "def contrast(x, severity=1):\n",
    "    c = [1.1, 1.2, 1.3, 1.4, 1.5] [severity-1]\n",
    "    x = np.array(x) / 255.\n",
    "    return np.clip((x - 128.0/255.0) * c + 128.0/255.0, 0, 1) * 255.\n",
    "\n",
    "\n",
    "\n",
    "# /////////////////////////////////////////////\n",
    "# /data/imagenet/ILSVRC/Data/CLS-LOC/real_train\n",
    "# /home/jtang/Desktop/val\n",
    "def save_distorted(method=gaussian_noise):\n",
    "    for severity in range(3, 4):\n",
    "        #severity = 0\n",
    "        print(method.__name__, severity)\n",
    "        distorted_dataset = DistortImageFolder(\n",
    "            root=\"/data/imagenet/ILSVRC/Data/CLS-LOC/real_train\",\n",
    "            method=method, severity=severity,\n",
    "            transform=trn.Compose([trn.Resize(256), trn.CenterCrop(224)]))\n",
    "        distorted_dataset_loader = torch.utils.data.DataLoader(\n",
    "            distorted_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "        for _ in distorted_dataset_loader: continue\n",
    "\n",
    "\n",
    "# /////////////// Display Results ///////////////\n",
    "import collections\n",
    "\n",
    "print('\\nUsing ImageNet data')\n",
    "save_distorted(contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "#/home/jtang/robustness/imagenet-r/DeepAugment/EDSR_Model\n",
    "# some_file.py\n",
    "#import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "#sys.path.insert(1, '/home/jtang/robustness/imagenet-r/DeepAugment/EDSR_Model')\n",
    "#from EDSR_Model import common\n",
    "\n",
    "#from .jtang.robustness.imagenet_r.DeepAugment.EDSR_Model import common\n",
    "from EDSR_Model import common\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as trn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.functional as trnF\n",
    "import numpy as np\n",
    "from torch.nn.functional import gelu\n",
    "from torch.nn.functional import conv2d\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets import ImageFolder\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1000 classes ['n01440764' 'n01443537' 'n01484850' 'n01491361' 'n01494475' 'n01496331'\n",
      " 'n01498041' 'n01514668' 'n01514859' 'n01518878' 'n01530575' 'n01531178'\n",
      " 'n01532829' 'n01534433' 'n01537544' 'n01558993' 'n01560419' 'n01580077'\n",
      " 'n01582220' 'n01592084' 'n01601694' 'n01608432' 'n01614925' 'n01616318'\n",
      " 'n01622779' 'n01629819' 'n01630670' 'n01631663' 'n01632458' 'n01632777'\n",
      " 'n01641577' 'n01644373' 'n01644900' 'n01664065' 'n01665541' 'n01667114'\n",
      " 'n01667778' 'n01669191' 'n01675722' 'n01677366' 'n01682714' 'n01685808'\n",
      " 'n01687978' 'n01688243' 'n01689811' 'n01692333' 'n01693334' 'n01694178'\n",
      " 'n01695060' 'n01697457' 'n01698640' 'n01704323' 'n01728572' 'n01728920'\n",
      " 'n01729322' 'n01729977' 'n01734418' 'n01735189' 'n01737021' 'n01739381'\n",
      " 'n01740131' 'n01742172' 'n01744401' 'n01748264' 'n01749939' 'n01751748'\n",
      " 'n01753488' 'n01755581' 'n01756291' 'n01768244' 'n01770081' 'n01770393'\n",
      " 'n01773157' 'n01773549' 'n01773797' 'n01774384' 'n01774750' 'n01775062'\n",
      " 'n01776313' 'n01784675' 'n01795545' 'n01796340' 'n01797886' 'n01798484'\n",
      " 'n01806143' 'n01806567' 'n01807496' 'n01817953' 'n01818515' 'n01819313'\n",
      " 'n01820546' 'n01824575' 'n01828970' 'n01829413' 'n01833805' 'n01843065'\n",
      " 'n01843383' 'n01847000' 'n01855032' 'n01855672' 'n01860187' 'n01871265'\n",
      " 'n01872401' 'n01873310' 'n01877812' 'n01882714' 'n01883070' 'n01910747'\n",
      " 'n01914609' 'n01917289' 'n01924916' 'n01930112' 'n01943899' 'n01944390'\n",
      " 'n01945685' 'n01950731' 'n01955084' 'n01968897' 'n01978287' 'n01978455'\n",
      " 'n01980166' 'n01981276' 'n01983481' 'n01984695' 'n01985128' 'n01986214'\n",
      " 'n01990800' 'n02002556' 'n02002724' 'n02006656' 'n02007558' 'n02009229'\n",
      " 'n02009912' 'n02011460' 'n02012849' 'n02013706' 'n02017213' 'n02018207'\n",
      " 'n02018795' 'n02025239' 'n02027492' 'n02028035' 'n02033041' 'n02037110'\n",
      " 'n02051845' 'n02056570' 'n02058221' 'n02066245' 'n02071294' 'n02074367'\n",
      " 'n02077923' 'n02085620' 'n02085782' 'n02085936' 'n02086079' 'n02086240'\n",
      " 'n02086646' 'n02086910' 'n02087046' 'n02087394' 'n02088094' 'n02088238'\n",
      " 'n02088364' 'n02088466' 'n02088632' 'n02089078' 'n02089867' 'n02089973'\n",
      " 'n02090379' 'n02090622' 'n02090721' 'n02091032' 'n02091134' 'n02091244'\n",
      " 'n02091467' 'n02091635' 'n02091831' 'n02092002' 'n02092339' 'n02093256'\n",
      " 'n02093428' 'n02093647' 'n02093754' 'n02093859' 'n02093991' 'n02094114'\n",
      " 'n02094258' 'n02094433' 'n02095314' 'n02095570' 'n02095889' 'n02096051'\n",
      " 'n02096177' 'n02096294' 'n02096437' 'n02096585' 'n02097047' 'n02097130'\n",
      " 'n02097209' 'n02097298' 'n02097474' 'n02097658' 'n02098105' 'n02098286'\n",
      " 'n02098413' 'n02099267' 'n02099429' 'n02099601' 'n02099712' 'n02099849'\n",
      " 'n02100236' 'n02100583' 'n02100735' 'n02100877' 'n02101006' 'n02101388'\n",
      " 'n02101556' 'n02102040' 'n02102177' 'n02102318' 'n02102480' 'n02102973'\n",
      " 'n02104029' 'n02104365' 'n02105056' 'n02105162' 'n02105251' 'n02105412'\n",
      " 'n02105505' 'n02105641' 'n02105855' 'n02106030' 'n02106166' 'n02106382'\n",
      " 'n02106550' 'n02106662' 'n02107142' 'n02107312' 'n02107574' 'n02107683'\n",
      " 'n02107908' 'n02108000' 'n02108089' 'n02108422' 'n02108551' 'n02108915'\n",
      " 'n02109047' 'n02109525' 'n02109961' 'n02110063' 'n02110185' 'n02110341'\n",
      " 'n02110627' 'n02110806' 'n02110958' 'n02111129' 'n02111277' 'n02111500'\n",
      " 'n02111889' 'n02112018' 'n02112137' 'n02112350' 'n02112706' 'n02113023'\n",
      " 'n02113186' 'n02113624' 'n02113712' 'n02113799' 'n02113978' 'n02114367'\n",
      " 'n02114548' 'n02114712' 'n02114855' 'n02115641' 'n02115913' 'n02116738'\n",
      " 'n02117135' 'n02119022' 'n02119789' 'n02120079' 'n02120505' 'n02123045'\n",
      " 'n02123159' 'n02123394' 'n02123597' 'n02124075' 'n02125311' 'n02127052'\n",
      " 'n02128385' 'n02128757' 'n02128925' 'n02129165' 'n02129604' 'n02130308'\n",
      " 'n02132136' 'n02133161' 'n02134084' 'n02134418' 'n02137549' 'n02138441'\n",
      " 'n02165105' 'n02165456' 'n02167151' 'n02168699' 'n02169497' 'n02172182'\n",
      " 'n02174001' 'n02177972' 'n02190166' 'n02206856' 'n02219486' 'n02226429'\n",
      " 'n02229544' 'n02231487' 'n02233338' 'n02236044' 'n02256656' 'n02259212'\n",
      " 'n02264363' 'n02268443' 'n02268853' 'n02276258' 'n02277742' 'n02279972'\n",
      " 'n02280649' 'n02281406' 'n02281787' 'n02317335' 'n02319095' 'n02321529'\n",
      " 'n02325366' 'n02326432' 'n02328150' 'n02342885' 'n02346627' 'n02356798'\n",
      " 'n02361337' 'n02363005' 'n02364673' 'n02389026' 'n02391049' 'n02395406'\n",
      " 'n02396427' 'n02397096' 'n02398521' 'n02403003' 'n02408429' 'n02410509'\n",
      " 'n02412080' 'n02415577' 'n02417914' 'n02422106' 'n02422699' 'n02423022'\n",
      " 'n02437312' 'n02437616' 'n02441942' 'n02442845' 'n02443114' 'n02443484'\n",
      " 'n02444819' 'n02445715' 'n02447366' 'n02454379' 'n02457408' 'n02480495'\n",
      " 'n02480855' 'n02481823' 'n02483362' 'n02483708' 'n02484975' 'n02486261'\n",
      " 'n02486410' 'n02487347' 'n02488291' 'n02488702' 'n02489166' 'n02490219'\n",
      " 'n02492035' 'n02492660' 'n02493509' 'n02493793' 'n02494079' 'n02497673'\n",
      " 'n02500267' 'n02504013' 'n02504458' 'n02509815' 'n02510455' 'n02514041'\n",
      " 'n02526121' 'n02536864' 'n02606052' 'n02607072' 'n02640242' 'n02641379'\n",
      " 'n02643566' 'n02655020' 'n02666196' 'n02667093' 'n02669723' 'n02672831'\n",
      " 'n02676566' 'n02687172' 'n02690373' 'n02692877' 'n02699494' 'n02701002'\n",
      " 'n02704792' 'n02708093' 'n02727426' 'n02730930' 'n02747177' 'n02749479'\n",
      " 'n02769748' 'n02776631' 'n02777292' 'n02782093' 'n02783161' 'n02786058'\n",
      " 'n02787622' 'n02788148' 'n02790996' 'n02791124' 'n02791270' 'n02793495'\n",
      " 'n02794156' 'n02795169' 'n02797295' 'n02799071' 'n02802426' 'n02804414'\n",
      " 'n02804610' 'n02807133' 'n02808304' 'n02808440' 'n02814533' 'n02814860'\n",
      " 'n02815834' 'n02817516' 'n02823428' 'n02823750' 'n02825657' 'n02834397'\n",
      " 'n02835271' 'n02837789' 'n02840245' 'n02841315' 'n02843684' 'n02859443'\n",
      " 'n02860847' 'n02865351' 'n02869837' 'n02870880' 'n02871525' 'n02877765'\n",
      " 'n02879718' 'n02883205' 'n02892201' 'n02892767' 'n02894605' 'n02895154'\n",
      " 'n02906734' 'n02909870' 'n02910353' 'n02916936' 'n02917067' 'n02927161'\n",
      " 'n02930766' 'n02939185' 'n02948072' 'n02950826' 'n02951358' 'n02951585'\n",
      " 'n02963159' 'n02965783' 'n02966193' 'n02966687' 'n02971356' 'n02974003'\n",
      " 'n02977058' 'n02978881' 'n02979186' 'n02980441' 'n02981792' 'n02988304'\n",
      " 'n02992211' 'n02992529' 'n02999410' 'n03000134' 'n03000247' 'n03000684'\n",
      " 'n03014705' 'n03016953' 'n03017168' 'n03018349' 'n03026506' 'n03028079'\n",
      " 'n03032252' 'n03041632' 'n03042490' 'n03045698' 'n03047690' 'n03062245'\n",
      " 'n03063599' 'n03063689' 'n03065424' 'n03075370' 'n03085013' 'n03089624'\n",
      " 'n03095699' 'n03100240' 'n03109150' 'n03110669' 'n03124043' 'n03124170'\n",
      " 'n03125729' 'n03126707' 'n03127747' 'n03127925' 'n03131574' 'n03133878'\n",
      " 'n03134739' 'n03141823' 'n03146219' 'n03160309' 'n03179701' 'n03180011'\n",
      " 'n03187595' 'n03188531' 'n03196217' 'n03197337' 'n03201208' 'n03207743'\n",
      " 'n03207941' 'n03208938' 'n03216828' 'n03218198' 'n03220513' 'n03223299'\n",
      " 'n03240683' 'n03249569' 'n03250847' 'n03255030' 'n03259280' 'n03271574'\n",
      " 'n03272010' 'n03272562' 'n03290653' 'n03291819' 'n03297495' 'n03314780'\n",
      " 'n03325584' 'n03337140' 'n03344393' 'n03345487' 'n03347037' 'n03355925'\n",
      " 'n03372029' 'n03376595' 'n03379051' 'n03384352' 'n03388043' 'n03388183'\n",
      " 'n03388549' 'n03393912' 'n03394916' 'n03400231' 'n03404251' 'n03417042'\n",
      " 'n03424325' 'n03425413' 'n03443371' 'n03444034' 'n03445777' 'n03445924'\n",
      " 'n03447447' 'n03447721' 'n03450230' 'n03452741' 'n03457902' 'n03459775'\n",
      " 'n03461385' 'n03467068' 'n03476684' 'n03476991' 'n03478589' 'n03481172'\n",
      " 'n03482405' 'n03483316' 'n03485407' 'n03485794' 'n03492542' 'n03494278'\n",
      " 'n03495258' 'n03496892' 'n03498962' 'n03527444' 'n03529860' 'n03530642'\n",
      " 'n03532672' 'n03534580' 'n03535780' 'n03538406' 'n03544143' 'n03584254'\n",
      " 'n03584829' 'n03590841' 'n03594734' 'n03594945' 'n03595614' 'n03598930'\n",
      " 'n03599486' 'n03602883' 'n03617480' 'n03623198' 'n03627232' 'n03630383'\n",
      " 'n03633091' 'n03637318' 'n03642806' 'n03649909' 'n03657121' 'n03658185'\n",
      " 'n03661043' 'n03662601' 'n03666591' 'n03670208' 'n03673027' 'n03676483'\n",
      " 'n03680355' 'n03690938' 'n03691459' 'n03692522' 'n03697007' 'n03706229'\n",
      " 'n03709823' 'n03710193' 'n03710637' 'n03710721' 'n03717622' 'n03720891'\n",
      " 'n03721384' 'n03724870' 'n03729826' 'n03733131' 'n03733281' 'n03733805'\n",
      " 'n03742115' 'n03743016' 'n03759954' 'n03761084' 'n03763968' 'n03764736'\n",
      " 'n03769881' 'n03770439' 'n03770679' 'n03773504' 'n03775071' 'n03775546'\n",
      " 'n03776460' 'n03777568' 'n03777754' 'n03781244' 'n03782006' 'n03785016'\n",
      " 'n03786901' 'n03787032' 'n03788195' 'n03788365' 'n03791053' 'n03792782'\n",
      " 'n03792972' 'n03793489' 'n03794056' 'n03796401' 'n03803284' 'n03804744'\n",
      " 'n03814639' 'n03814906' 'n03825788' 'n03832673' 'n03837869' 'n03838899'\n",
      " 'n03840681' 'n03841143' 'n03843555' 'n03854065' 'n03857828' 'n03866082'\n",
      " 'n03868242' 'n03868863' 'n03871628' 'n03873416' 'n03874293' 'n03874599'\n",
      " 'n03876231' 'n03877472' 'n03877845' 'n03884397' 'n03887697' 'n03888257'\n",
      " 'n03888605' 'n03891251' 'n03891332' 'n03895866' 'n03899768' 'n03902125'\n",
      " 'n03903868' 'n03908618' 'n03908714' 'n03916031' 'n03920288' 'n03924679'\n",
      " 'n03929660' 'n03929855' 'n03930313' 'n03930630' 'n03933933' 'n03935335'\n",
      " 'n03937543' 'n03938244' 'n03942813' 'n03944341' 'n03947888' 'n03950228'\n",
      " 'n03954731' 'n03956157' 'n03958227' 'n03961711' 'n03967562' 'n03970156'\n",
      " 'n03976467' 'n03976657' 'n03977966' 'n03980874' 'n03982430' 'n03983396'\n",
      " 'n03991062' 'n03992509' 'n03995372' 'n03998194' 'n04004767' 'n04005630'\n",
      " 'n04008634' 'n04009552' 'n04019541' 'n04023962' 'n04026417' 'n04033901'\n",
      " 'n04033995' 'n04037443' 'n04039381' 'n04040759' 'n04041544' 'n04044716'\n",
      " 'n04049303' 'n04065272' 'n04067472' 'n04069434' 'n04070727' 'n04074963'\n",
      " 'n04081281' 'n04086273' 'n04090263' 'n04099969' 'n04111531' 'n04116512'\n",
      " 'n04118538' 'n04118776' 'n04120489' 'n04125021' 'n04127249' 'n04131690'\n",
      " 'n04133789' 'n04136333' 'n04141076' 'n04141327' 'n04141975' 'n04146614'\n",
      " 'n04147183' 'n04149813' 'n04152593' 'n04153751' 'n04154565' 'n04162706'\n",
      " 'n04179913' 'n04192698' 'n04200800' 'n04201297' 'n04204238' 'n04204347'\n",
      " 'n04208210' 'n04209133' 'n04209239' 'n04228054' 'n04229816' 'n04235860'\n",
      " 'n04238763' 'n04239074' 'n04243546' 'n04251144' 'n04252077' 'n04252225'\n",
      " 'n04254120' 'n04254680' 'n04254777' 'n04258138' 'n04259630' 'n04263257'\n",
      " 'n04264628' 'n04265275' 'n04266014' 'n04270147' 'n04273569' 'n04275548'\n",
      " 'n04277352' 'n04285008' 'n04286575' 'n04296562' 'n04310018' 'n04311004'\n",
      " 'n04311174' 'n04317175' 'n04325704' 'n04326547' 'n04328186' 'n04330267'\n",
      " 'n04332243' 'n04335435' 'n04336792' 'n04344873' 'n04346328' 'n04347754'\n",
      " 'n04350905' 'n04355338' 'n04355933' 'n04356056' 'n04357314' 'n04366367'\n",
      " 'n04367480' 'n04370456' 'n04371430' 'n04371774' 'n04372370' 'n04376876'\n",
      " 'n04380533' 'n04389033' 'n04392985' 'n04398044' 'n04399382' 'n04404412'\n",
      " 'n04409515' 'n04417672' 'n04418357' 'n04423845' 'n04428191' 'n04429376'\n",
      " 'n04435653' 'n04442312' 'n04443257' 'n04447861' 'n04456115' 'n04458633'\n",
      " 'n04461696' 'n04462240' 'n04465501' 'n04467665' 'n04476259' 'n04479046'\n",
      " 'n04482393' 'n04483307' 'n04485082' 'n04486054' 'n04487081' 'n04487394'\n",
      " 'n04493381' 'n04501370' 'n04505470' 'n04507155' 'n04509417' 'n04515003'\n",
      " 'n04517823' 'n04522168' 'n04523525' 'n04525038' 'n04525305' 'n04532106'\n",
      " 'n04532670' 'n04536866' 'n04540053' 'n04542943' 'n04548280' 'n04548362'\n",
      " 'n04550184' 'n04552348' 'n04553703' 'n04554684' 'n04557648' 'n04560804'\n",
      " 'n04562935' 'n04579145' 'n04579432' 'n04584207' 'n04589890' 'n04590129'\n",
      " 'n04591157' 'n04591713' 'n04592741' 'n04596742' 'n04597913' 'n04599235'\n",
      " 'n04604644' 'n04606251' 'n04612504' 'n04613696' 'n06359193' 'n06596364'\n",
      " 'n06785654' 'n06794110' 'n06874185' 'n07248320' 'n07565083' 'n07579787'\n",
      " 'n07583066' 'n07584110' 'n07590611' 'n07613480' 'n07614500' 'n07615774'\n",
      " 'n07684084' 'n07693725' 'n07695742' 'n07697313' 'n07697537' 'n07711569'\n",
      " 'n07714571' 'n07714990' 'n07715103' 'n07716358' 'n07716906' 'n07717410'\n",
      " 'n07717556' 'n07718472' 'n07718747' 'n07720875' 'n07730033' 'n07734744'\n",
      " 'n07742313' 'n07745940' 'n07747607' 'n07749582' 'n07753113' 'n07753275'\n",
      " 'n07753592' 'n07754684' 'n07760859' 'n07768694' 'n07802026' 'n07831146'\n",
      " 'n07836838' 'n07860988' 'n07871810' 'n07873807' 'n07875152' 'n07880968'\n",
      " 'n07892512' 'n07920052' 'n07930864' 'n07932039' 'n09193705' 'n09229709'\n",
      " 'n09246464' 'n09256479' 'n09288635' 'n09332890' 'n09399592' 'n09421951'\n",
      " 'n09428293' 'n09468604' 'n09472597' 'n09835506' 'n10148035' 'n10565667'\n",
      " 'n11879895' 'n11939491' 'n12057211' 'n12144580' 'n12267677' 'n12620546'\n",
      " 'n12768682' 'n12985857' 'n12998815' 'n13037406' 'n13040303' 'n13044778'\n",
      " 'n13052670' 'n13054560' 'n13133613' 'n15075141']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [00:57<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "#parser = argparse.ArgumentParser(description='Fine-tune')\n",
    "#parser.add_argument('--total-workers', default=None, type=int, required=True)\n",
    "#parser.add_argument('--worker-number', default=None, type=int, required=True) # MUST BE 0-INDEXED\n",
    "#args = parser.parse_args()\n",
    "total_workers = 1\n",
    "worker_number = 0\n",
    "\n",
    "all_classes = [\"n01440764\", \"n01443537\", \"n01484850\", \"n01491361\", \"n01494475\", \"n01496331\", \"n01498041\", \"n01514668\", \"n01514859\", \"n01518878\", \"n01530575\", \"n01531178\", \"n01532829\", \"n01534433\", \"n01537544\", \"n01558993\", \"n01560419\", \"n01580077\", \"n01582220\", \"n01592084\", \"n01601694\", \"n01608432\", \"n01614925\", \"n01616318\", \"n01622779\", \"n01629819\", \"n01630670\", \"n01631663\", \"n01632458\", \"n01632777\", \"n01641577\", \"n01644373\", \"n01644900\", \"n01664065\", \"n01665541\", \"n01667114\", \"n01667778\", \"n01669191\", \"n01675722\", \"n01677366\", \"n01682714\", \"n01685808\", \"n01687978\", \"n01688243\", \"n01689811\", \"n01692333\", \"n01693334\", \"n01694178\", \"n01695060\", \"n01697457\", \"n01698640\", \"n01704323\", \"n01728572\", \"n01728920\", \"n01729322\", \"n01729977\", \"n01734418\", \"n01735189\", \"n01737021\", \"n01739381\", \"n01740131\", \"n01742172\", \"n01744401\", \"n01748264\", \"n01749939\", \"n01751748\", \"n01753488\", \"n01755581\", \"n01756291\", \"n01768244\", \"n01770081\", \"n01770393\", \"n01773157\", \"n01773549\", \"n01773797\", \"n01774384\", \"n01774750\", \"n01775062\", \"n01776313\", \"n01784675\", \"n01795545\", \"n01796340\", \"n01797886\", \"n01798484\", \"n01806143\", \"n01806567\", \"n01807496\", \"n01817953\", \"n01818515\", \"n01819313\", \"n01820546\", \"n01824575\", \"n01828970\", \"n01829413\", \"n01833805\", \"n01843065\", \"n01843383\", \"n01847000\", \"n01855032\", \"n01855672\", \"n01860187\", \"n01871265\", \"n01872401\", \"n01873310\", \"n01877812\", \"n01882714\", \"n01883070\", \"n01910747\", \"n01914609\", \"n01917289\", \"n01924916\", \"n01930112\", \"n01943899\", \"n01944390\", \"n01945685\", \"n01950731\", \"n01955084\", \"n01968897\", \"n01978287\", \"n01978455\", \"n01980166\", \"n01981276\", \"n01983481\", \"n01984695\", \"n01985128\", \"n01986214\", \"n01990800\", \"n02002556\", \"n02002724\", \"n02006656\", \"n02007558\", \"n02009229\", \"n02009912\", \"n02011460\", \"n02012849\", \"n02013706\", \"n02017213\", \"n02018207\", \"n02018795\", \"n02025239\", \"n02027492\", \"n02028035\", \"n02033041\", \"n02037110\", \"n02051845\", \"n02056570\", \"n02058221\", \"n02066245\", \"n02071294\", \"n02074367\", \"n02077923\", \"n02085620\", \"n02085782\", \"n02085936\", \"n02086079\", \"n02086240\", \"n02086646\", \"n02086910\", \"n02087046\", \"n02087394\", \"n02088094\", \"n02088238\", \"n02088364\", \"n02088466\", \"n02088632\", \"n02089078\", \"n02089867\", \"n02089973\", \"n02090379\", \"n02090622\", \"n02090721\", \"n02091032\", \"n02091134\", \"n02091244\", \"n02091467\", \"n02091635\", \"n02091831\", \"n02092002\", \"n02092339\", \"n02093256\", \"n02093428\", \"n02093647\", \"n02093754\", \"n02093859\", \"n02093991\", \"n02094114\", \"n02094258\", \"n02094433\", \"n02095314\", \"n02095570\", \"n02095889\", \"n02096051\", \"n02096177\", \"n02096294\", \"n02096437\", \"n02096585\", \"n02097047\", \"n02097130\", \"n02097209\", \"n02097298\", \"n02097474\", \"n02097658\", \"n02098105\", \"n02098286\", \"n02098413\", \"n02099267\", \"n02099429\", \"n02099601\", \"n02099712\", \"n02099849\", \"n02100236\", \"n02100583\", \"n02100735\", \"n02100877\", \"n02101006\", \"n02101388\", \"n02101556\", \"n02102040\", \"n02102177\", \"n02102318\", \"n02102480\", \"n02102973\", \"n02104029\", \"n02104365\", \"n02105056\", \"n02105162\", \"n02105251\", \"n02105412\", \"n02105505\", \"n02105641\", \"n02105855\", \"n02106030\", \"n02106166\", \"n02106382\", \"n02106550\", \"n02106662\", \"n02107142\", \"n02107312\", \"n02107574\", \"n02107683\", \"n02107908\", \"n02108000\", \"n02108089\", \"n02108422\", \"n02108551\", \"n02108915\", \"n02109047\", \"n02109525\", \"n02109961\", \"n02110063\", \"n02110185\", \"n02110341\", \"n02110627\", \"n02110806\", \"n02110958\", \"n02111129\", \"n02111277\", \"n02111500\", \"n02111889\", \"n02112018\", \"n02112137\", \"n02112350\", \"n02112706\", \"n02113023\", \"n02113186\", \"n02113624\", \"n02113712\", \"n02113799\", \"n02113978\", \"n02114367\", \"n02114548\", \"n02114712\", \"n02114855\", \"n02115641\", \"n02115913\", \"n02116738\", \"n02117135\", \"n02119022\", \"n02119789\", \"n02120079\", \"n02120505\", \"n02123045\", \"n02123159\", \"n02123394\", \"n02123597\", \"n02124075\", \"n02125311\", \"n02127052\", \"n02128385\", \"n02128757\", \"n02128925\", \"n02129165\", \"n02129604\", \"n02130308\", \"n02132136\", \"n02133161\", \"n02134084\", \"n02134418\", \"n02137549\", \"n02138441\", \"n02165105\", \"n02165456\", \"n02167151\", \"n02168699\", \"n02169497\", \"n02172182\", \"n02174001\", \"n02177972\", \"n02190166\", \"n02206856\", \"n02219486\", \"n02226429\", \"n02229544\", \"n02231487\", \"n02233338\", \"n02236044\", \"n02256656\", \"n02259212\", \"n02264363\", \"n02268443\", \"n02268853\", \"n02276258\", \"n02277742\", \"n02279972\", \"n02280649\", \"n02281406\", \"n02281787\", \"n02317335\", \"n02319095\", \"n02321529\", \"n02325366\", \"n02326432\", \"n02328150\", \"n02342885\", \"n02346627\", \"n02356798\", \"n02361337\", \"n02363005\", \"n02364673\", \"n02389026\", \"n02391049\", \"n02395406\", \"n02396427\", \"n02397096\", \"n02398521\", \"n02403003\", \"n02408429\", \"n02410509\", \"n02412080\", \"n02415577\", \"n02417914\", \"n02422106\", \"n02422699\", \"n02423022\", \"n02437312\", \"n02437616\", \"n02441942\", \"n02442845\", \"n02443114\", \"n02443484\", \"n02444819\", \"n02445715\", \"n02447366\", \"n02454379\", \"n02457408\", \"n02480495\", \"n02480855\", \"n02481823\", \"n02483362\", \"n02483708\", \"n02484975\", \"n02486261\", \"n02486410\", \"n02487347\", \"n02488291\", \"n02488702\", \"n02489166\", \"n02490219\", \"n02492035\", \"n02492660\", \"n02493509\", \"n02493793\", \"n02494079\", \"n02497673\", \"n02500267\", \"n02504013\", \"n02504458\", \"n02509815\", \"n02510455\", \"n02514041\", \"n02526121\", \"n02536864\", \"n02606052\", \"n02607072\", \"n02640242\", \"n02641379\", \"n02643566\", \"n02655020\", \"n02666196\", \"n02667093\", \"n02669723\", \"n02672831\", \"n02676566\", \"n02687172\", \"n02690373\", \"n02692877\", \"n02699494\", \"n02701002\", \"n02704792\", \"n02708093\", \"n02727426\", \"n02730930\", \"n02747177\", \"n02749479\", \"n02769748\", \"n02776631\", \"n02777292\", \"n02782093\", \"n02783161\", \"n02786058\", \"n02787622\", \"n02788148\", \"n02790996\", \"n02791124\", \"n02791270\", \"n02793495\", \"n02794156\", \"n02795169\", \"n02797295\", \"n02799071\", \"n02802426\", \"n02804414\", \"n02804610\", \"n02807133\", \"n02808304\", \"n02808440\", \"n02814533\", \"n02814860\", \"n02815834\", \"n02817516\", \"n02823428\", \"n02823750\", \"n02825657\", \"n02834397\", \"n02835271\", \"n02837789\", \"n02840245\", \"n02841315\", \"n02843684\", \"n02859443\", \"n02860847\", \"n02865351\", \"n02869837\", \"n02870880\", \"n02871525\", \"n02877765\", \"n02879718\", \"n02883205\", \"n02892201\", \"n02892767\", \"n02894605\", \"n02895154\", \"n02906734\", \"n02909870\", \"n02910353\", \"n02916936\", \"n02917067\", \"n02927161\", \"n02930766\", \"n02939185\", \"n02948072\", \"n02950826\", \"n02951358\", \"n02951585\", \"n02963159\", \"n02965783\", \"n02966193\", \"n02966687\", \"n02971356\", \"n02974003\", \"n02977058\", \"n02978881\", \"n02979186\", \"n02980441\", \"n02981792\", \"n02988304\", \"n02992211\", \"n02992529\", \"n02999410\", \"n03000134\", \"n03000247\", \"n03000684\", \"n03014705\", \"n03016953\", \"n03017168\", \"n03018349\", \"n03026506\", \"n03028079\", \"n03032252\", \"n03041632\", \"n03042490\", \"n03045698\", \"n03047690\", \"n03062245\", \"n03063599\", \"n03063689\", \"n03065424\", \"n03075370\", \"n03085013\", \"n03089624\", \"n03095699\", \"n03100240\", \"n03109150\", \"n03110669\", \"n03124043\", \"n03124170\", \"n03125729\", \"n03126707\", \"n03127747\", \"n03127925\", \"n03131574\", \"n03133878\", \"n03134739\", \"n03141823\", \"n03146219\", \"n03160309\", \"n03179701\", \"n03180011\", \"n03187595\", \"n03188531\", \"n03196217\", \"n03197337\", \"n03201208\", \"n03207743\", \"n03207941\", \"n03208938\", \"n03216828\", \"n03218198\", \"n03220513\", \"n03223299\", \"n03240683\", \"n03249569\", \"n03250847\", \"n03255030\", \"n03259280\", \"n03271574\", \"n03272010\", \"n03272562\", \"n03290653\", \"n03291819\", \"n03297495\", \"n03314780\", \"n03325584\", \"n03337140\", \"n03344393\", \"n03345487\", \"n03347037\", \"n03355925\", \"n03372029\", \"n03376595\", \"n03379051\", \"n03384352\", \"n03388043\", \"n03388183\", \"n03388549\", \"n03393912\", \"n03394916\", \"n03400231\", \"n03404251\", \"n03417042\", \"n03424325\", \"n03425413\", \"n03443371\", \"n03444034\", \"n03445777\", \"n03445924\", \"n03447447\", \"n03447721\", \"n03450230\", \"n03452741\", \"n03457902\", \"n03459775\", \"n03461385\", \"n03467068\", \"n03476684\", \"n03476991\", \"n03478589\", \"n03481172\", \"n03482405\", \"n03483316\", \"n03485407\", \"n03485794\", \"n03492542\", \"n03494278\", \"n03495258\", \"n03496892\", \"n03498962\", \"n03527444\", \"n03529860\", \"n03530642\", \"n03532672\", \"n03534580\", \"n03535780\", \"n03538406\", \"n03544143\", \"n03584254\", \"n03584829\", \"n03590841\", \"n03594734\", \"n03594945\", \"n03595614\", \"n03598930\", \"n03599486\", \"n03602883\", \"n03617480\", \"n03623198\", \"n03627232\", \"n03630383\", \"n03633091\", \"n03637318\", \"n03642806\", \"n03649909\", \"n03657121\", \"n03658185\", \"n03661043\", \"n03662601\", \"n03666591\", \"n03670208\", \"n03673027\", \"n03676483\", \"n03680355\", \"n03690938\", \"n03691459\", \"n03692522\", \"n03697007\", \"n03706229\", \"n03709823\", \"n03710193\", \"n03710637\", \"n03710721\", \"n03717622\", \"n03720891\", \"n03721384\", \"n03724870\", \"n03729826\", \"n03733131\", \"n03733281\", \"n03733805\", \"n03742115\", \"n03743016\", \"n03759954\", \"n03761084\", \"n03763968\", \"n03764736\", \"n03769881\", \"n03770439\", \"n03770679\", \"n03773504\", \"n03775071\", \"n03775546\", \"n03776460\", \"n03777568\", \"n03777754\", \"n03781244\", \"n03782006\", \"n03785016\", \"n03786901\", \"n03787032\", \"n03788195\", \"n03788365\", \"n03791053\", \"n03792782\", \"n03792972\", \"n03793489\", \"n03794056\", \"n03796401\", \"n03803284\", \"n03804744\", \"n03814639\", \"n03814906\", \"n03825788\", \"n03832673\", \"n03837869\", \"n03838899\", \"n03840681\", \"n03841143\", \"n03843555\", \"n03854065\", \"n03857828\", \"n03866082\", \"n03868242\", \"n03868863\", \"n03871628\", \"n03873416\", \"n03874293\", \"n03874599\", \"n03876231\", \"n03877472\", \"n03877845\", \"n03884397\", \"n03887697\", \"n03888257\", \"n03888605\", \"n03891251\", \"n03891332\", \"n03895866\", \"n03899768\", \"n03902125\", \"n03903868\", \"n03908618\", \"n03908714\", \"n03916031\", \"n03920288\", \"n03924679\", \"n03929660\", \"n03929855\", \"n03930313\", \"n03930630\", \"n03933933\", \"n03935335\", \"n03937543\", \"n03938244\", \"n03942813\", \"n03944341\", \"n03947888\", \"n03950228\", \"n03954731\", \"n03956157\", \"n03958227\", \"n03961711\", \"n03967562\", \"n03970156\", \"n03976467\", \"n03976657\", \"n03977966\", \"n03980874\", \"n03982430\", \"n03983396\", \"n03991062\", \"n03992509\", \"n03995372\", \"n03998194\", \"n04004767\", \"n04005630\", \"n04008634\", \"n04009552\", \"n04019541\", \"n04023962\", \"n04026417\", \"n04033901\", \"n04033995\", \"n04037443\", \"n04039381\", \"n04040759\", \"n04041544\", \"n04044716\", \"n04049303\", \"n04065272\", \"n04067472\", \"n04069434\", \"n04070727\", \"n04074963\", \"n04081281\", \"n04086273\", \"n04090263\", \"n04099969\", \"n04111531\", \"n04116512\", \"n04118538\", \"n04118776\", \"n04120489\", \"n04125021\", \"n04127249\", \"n04131690\", \"n04133789\", \"n04136333\", \"n04141076\", \"n04141327\", \"n04141975\", \"n04146614\", \"n04147183\", \"n04149813\", \"n04152593\", \"n04153751\", \"n04154565\", \"n04162706\", \"n04179913\", \"n04192698\", \"n04200800\", \"n04201297\", \"n04204238\", \"n04204347\", \"n04208210\", \"n04209133\", \"n04209239\", \"n04228054\", \"n04229816\", \"n04235860\", \"n04238763\", \"n04239074\", \"n04243546\", \"n04251144\", \"n04252077\", \"n04252225\", \"n04254120\", \"n04254680\", \"n04254777\", \"n04258138\", \"n04259630\", \"n04263257\", \"n04264628\", \"n04265275\", \"n04266014\", \"n04270147\", \"n04273569\", \"n04275548\", \"n04277352\", \"n04285008\", \"n04286575\", \"n04296562\", \"n04310018\", \"n04311004\", \"n04311174\", \"n04317175\", \"n04325704\", \"n04326547\", \"n04328186\", \"n04330267\", \"n04332243\", \"n04335435\", \"n04336792\", \"n04344873\", \"n04346328\", \"n04347754\", \"n04350905\", \"n04355338\", \"n04355933\", \"n04356056\", \"n04357314\", \"n04366367\", \"n04367480\", \"n04370456\", \"n04371430\", \"n04371774\", \"n04372370\", \"n04376876\", \"n04380533\", \"n04389033\", \"n04392985\", \"n04398044\", \"n04399382\", \"n04404412\", \"n04409515\", \"n04417672\", \"n04418357\", \"n04423845\", \"n04428191\", \"n04429376\", \"n04435653\", \"n04442312\", \"n04443257\", \"n04447861\", \"n04456115\", \"n04458633\", \"n04461696\", \"n04462240\", \"n04465501\", \"n04467665\", \"n04476259\", \"n04479046\", \"n04482393\", \"n04483307\", \"n04485082\", \"n04486054\", \"n04487081\", \"n04487394\", \"n04493381\", \"n04501370\", \"n04505470\", \"n04507155\", \"n04509417\", \"n04515003\", \"n04517823\", \"n04522168\", \"n04523525\", \"n04525038\", \"n04525305\", \"n04532106\", \"n04532670\", \"n04536866\", \"n04540053\", \"n04542943\", \"n04548280\", \"n04548362\", \"n04550184\", \"n04552348\", \"n04553703\", \"n04554684\", \"n04557648\", \"n04560804\", \"n04562935\", \"n04579145\", \"n04579432\", \"n04584207\", \"n04589890\", \"n04590129\", \"n04591157\", \"n04591713\", \"n04592741\", \"n04596742\", \"n04597913\", \"n04599235\", \"n04604644\", \"n04606251\", \"n04612504\", \"n04613696\", \"n06359193\", \"n06596364\", \"n06785654\", \"n06794110\", \"n06874185\", \"n07248320\", \"n07565083\", \"n07579787\", \"n07583066\", \"n07584110\", \"n07590611\", \"n07613480\", \"n07614500\", \"n07615774\", \"n07684084\", \"n07693725\", \"n07695742\", \"n07697313\", \"n07697537\", \"n07711569\", \"n07714571\", \"n07714990\", \"n07715103\", \"n07716358\", \"n07716906\", \"n07717410\", \"n07717556\", \"n07718472\", \"n07718747\", \"n07720875\", \"n07730033\", \"n07734744\", \"n07742313\", \"n07745940\", \"n07747607\", \"n07749582\", \"n07753113\", \"n07753275\", \"n07753592\", \"n07754684\", \"n07760859\", \"n07768694\", \"n07802026\", \"n07831146\", \"n07836838\", \"n07860988\", \"n07871810\", \"n07873807\", \"n07875152\", \"n07880968\", \"n07892512\", \"n07920052\", \"n07930864\", \"n07932039\", \"n09193705\", \"n09229709\", \"n09246464\", \"n09256479\", \"n09288635\", \"n09332890\", \"n09399592\", \"n09421951\", \"n09428293\", \"n09468604\", \"n09472597\", \"n09835506\", \"n10148035\", \"n10565667\", \"n11879895\", \"n11939491\", \"n12057211\", \"n12144580\", \"n12267677\", \"n12620546\", \"n12768682\", \"n12985857\", \"n12998815\", \"n13037406\", \"n13040303\", \"n13044778\", \"n13052670\", \"n13054560\", \"n13133613\", \"n15075141\"]\n",
    "all_classes.sort()\n",
    "assert len(all_classes) == 1000\n",
    "\n",
    "# Subset for this worker\n",
    "classes_chosen = np.array_split(all_classes, total_workers)[worker_number]\n",
    "\n",
    "class ImageNetSubsetDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class to take a specified subset of some larger dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, root, *args, **kwargs):\n",
    "\n",
    "        print(\"Using {0} classes {1}\".format(len(classes_chosen), classes_chosen))\n",
    "\n",
    "        self.new_root = tempfile.mkdtemp()\n",
    "        for _class in classes_chosen:\n",
    "            orig_dir = os.path.join(root, _class)\n",
    "            #assert os.path.isdir(orig_dir)\n",
    "\n",
    "            os.symlink(orig_dir, os.path.join(self.new_root, _class))\n",
    "\n",
    "        super().__init__(self.new_root, *args, **kwargs)\n",
    "\n",
    "        return self.new_root\n",
    "\n",
    "    def __del__(self):\n",
    "        # Clean up\n",
    "        shutil.rmtree(self.new_root)\n",
    "\n",
    "\n",
    "# test_transform = lambda x: trnF.to_tensor(trnF.resize(x, 128))\n",
    "test_transform = trn.Compose([trn.Resize(128), trn.ToTensor()])\n",
    "\n",
    "\n",
    "def make_model(args, parent=False):\n",
    "    return EDSR(args)\n",
    "\n",
    "class EDSR(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_resblocks=16, n_feats=64, scale=4, res_scale=1, rgb_range=255, n_colors=3,\n",
    "                 conv=common.default_conv):\n",
    "        super(EDSR, self).__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        act = nn.ReLU(True)\n",
    "        url_name = 'r{}f{}x{}'.format(n_resblocks, n_feats, scale)\n",
    "        self.sub_mean = common.MeanShift(rgb_range)\n",
    "        self.add_mean = common.MeanShift(rgb_range, sign=1)\n",
    "\n",
    "        # define head module\n",
    "        m_head = [conv(n_colors, n_feats, kernel_size)]\n",
    "\n",
    "        # define body module\n",
    "        m_body = [\n",
    "            common.ResBlock(\n",
    "                conv, n_feats, kernel_size, act=act, res_scale=res_scale\n",
    "            ) for _ in range(n_resblocks)\n",
    "        ]\n",
    "        m_body.append(conv(n_feats, n_feats, kernel_size))\n",
    "\n",
    "        # define tail module\n",
    "        m_tail = [\n",
    "            common.Upsampler(conv, scale, n_feats, act=False),\n",
    "            conv(n_feats, n_colors, kernel_size)\n",
    "        ]\n",
    "\n",
    "        self.head = nn.Sequential(*m_head)\n",
    "        self.body = nn.Sequential(*m_body)\n",
    "        self.tail = nn.Sequential(*m_tail)\n",
    "\n",
    "    def forward(self, x, pre_distortions={None}, body_distortions={None}):\n",
    "        # print(\"Using FF pre distortions = \", pre_distortions)\n",
    "        # print(\"Using FF body distortions = \", body_distortions)\n",
    "        x = self.sub_mean(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        ######################################################################\n",
    "        # PRE - DISTORTIONS\n",
    "        ######################################################################\n",
    "\n",
    "        if 1 in pre_distortions:\n",
    "            for _ in range(5):\n",
    "                c1, c2 = random.randint(0, 63), random.randint(0, 63)\n",
    "                x[:, c1], x[:, c2] = x[:, c2], x[:, c1]\n",
    "\n",
    "        if 2 in pre_distortions:\n",
    "            rand_filter_weight = torch.round(torch.rand_like(x) + 0.45) # Random matrix of 1s and 0s\n",
    "            x = x * rand_filter_weight\n",
    "\n",
    "        if 3 in pre_distortions:\n",
    "            rand_filter_weight = (torch.round(torch.rand_like(x) + 0.475) * 2) - 1 # Random matrix of 1s and -1s\n",
    "            x = x * rand_filter_weight\n",
    "\n",
    "        ######################################################################\n",
    "        # BODY - DISTORTIONS\n",
    "        ######################################################################\n",
    "\n",
    "        if 1 in body_distortions:\n",
    "            res = self.body[:5](x)\n",
    "            res = -res\n",
    "            res = self.body[5:](res)\n",
    "        elif 2 in body_distortions:\n",
    "            if random.randint(0, 2) == 1:\n",
    "                act = F.relu\n",
    "            else:\n",
    "                act = F.gelu\n",
    "            res = self.body[:5](x)\n",
    "            res = act(res)\n",
    "            res = self.body[5:](res)\n",
    "        elif 3 in body_distortions:\n",
    "            if random.randint(0, 2) == 1:\n",
    "                axes = [1, 2]\n",
    "            else:\n",
    "                axes = [1, 3]\n",
    "            res = self.body[:5](x)\n",
    "            res = torch.flip(res, axes)\n",
    "            res = self.body[5:](res)\n",
    "        elif 4 in body_distortions:\n",
    "            to_skip = set([random.randint(2, 16) for _ in range(3)])\n",
    "            for i in range(len(self.body)):\n",
    "                if i not in to_skip:\n",
    "                    res = self.body[i](x)\n",
    "        else:\n",
    "            res = self.body(x)\n",
    "\n",
    "        res += x\n",
    "\n",
    "        x = self.tail(res)\n",
    "        x = self.add_mean(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_weights():\n",
    "    weights = torch.load('./EDSR_Weights/edsr_baseline_x4.pt')\n",
    "\n",
    "    random_sample_list = np.random.randint(0,17, size=3)\n",
    "    for option in list(random_sample_list):\n",
    "        if option == 0:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = torch.flip(weights['body.'+str(i)+'.body.0.weight'], (0,))\n",
    "            weights['body.'+str(i)+'.body.0.bias'] = torch.flip(weights['body.'+str(i)+'.body.0.bias'], (0,))\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = torch.flip(weights['body.'+str(i)+'.body.2.weight'], (0,))\n",
    "            weights['body.'+str(i)+'.body.2.bias'] = torch.flip(weights['body.'+str(i)+'.body.2.bias'], (0,))\n",
    "        elif option == 1:\n",
    "            i = np.random.choice(np.arange(1,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = -weights['body.'+str(i)+'.body.0.weight']\n",
    "            weights['body.'+str(i)+'.body.0.bias'] = -weights['body.'+str(i)+'.body.0.bias']\n",
    "        elif option == 2:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = 0 * weights['body.'+str(i)+'.body.0.weight']\n",
    "            weights['body.'+str(i)+'.body.0.bias'] = 0*weights['body.'+str(i)+'.body.0.bias']\n",
    "        elif option == 3:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = -gelu(weights['body.'+str(i)+'.body.0.weight'])\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = -gelu(weights['body.'+str(i)+'.body.2.weight'])\n",
    "        elif option == 4:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = weights['body.'+str(i)+'.body.0.weight'] *\\\n",
    "            torch.Tensor([[0, 1, 0],[1, -4., 1], [0, 1, 0]]).view(1,1,3,3).cuda()\n",
    "        elif option == 5:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = weights['body.'+str(i)+'.body.0.weight'] *\\\n",
    "            torch.Tensor([[-1, -1, -1],[-1, 8., -1], [-1, -1, -1]]).view(1,1,3,3).cuda()\n",
    "        elif option == 6:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = weights['body.'+str(i)+'.body.2.weight'] *\\\n",
    "            (1 + 2 * np.float32(np.random.uniform()) * (2*torch.rand_like(weights['body.'+str(i)+'.body.2.weight']-1)))\n",
    "        elif option == 7:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = torch.flip(weights['body.'+str(i)+'.body.0.weight'], (-1,))\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = -1 * weights['body.'+str(i)+'.body.2.weight']\n",
    "        elif option == 8:\n",
    "            i = np.random.choice(np.arange(1,13,4))\n",
    "            z = torch.zeros_like(weights['body.'+str(i)+'.body.0.weight'])\n",
    "            for j in range(z.size(0)):\n",
    "                shift_x, shift_y = np.random.randint(3, size=(2,))\n",
    "                z[:,j,shift_x,shift_y] = np.random.choice([1.,-1.])\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = conv2d(weights['body.'+str(i)+'.body.0.weight'], z, padding=1)\n",
    "        elif option == 9:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            z = (2*torch.rand_like(weights['body.'+str(i)+'.body.0.weight'])*np.float32(np.random.uniform()) - 1)/6.\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = conv2d(weights['body.'+str(i)+'.body.0.weight'], z, padding=1)\n",
    "        elif option == 10:\n",
    "            i = np.random.choice(np.arange(1,12,4))\n",
    "            z = torch.FloatTensor(np.random.dirichlet([0.1] * 9, (64,64))).view(64,64,3,3).cuda() # 2.weight\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = conv2d(weights['body.'+str(i)+'.body.2.weight'], z, padding=1)\n",
    "        elif option == 11: ############ Start Saurav's changes ############\n",
    "            i = random.choice(list(range(15)))\n",
    "            noise = (torch.rand_like(weights['body.'+str(i)+'.body.2.weight']) - 0.5) * 1.0\n",
    "            weights['body.'+str(i)+'.body.2.weight'] += noise\n",
    "        elif option == 12:\n",
    "            _ij = [[random.choice(list(range(15))), random.choice([0, 2])] for _ in range(5)]\n",
    "            for i, j in _ij:\n",
    "                _k = random.randint(1, 3)\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    _dims = (2,3)\n",
    "                else:\n",
    "                    _dims = (0,1)\n",
    "                weights['body.'+str(i)+'.body.'+str(j)+'.weight'] = torch.rot90(weights['body.'+str(i)+'.body.'+str(j)+'.weight'], k=_k, dims=_dims)\n",
    "        elif option == 13:\n",
    "            _i = [random.choice(list(range(15))) for _ in range(5)]\n",
    "            for i in _i:\n",
    "                rand_filter_weight = torch.round(torch.rand_like(weights['body.'+str(i)+'.body.0.weight'])) * 2 - 1 # Random matrix of 1s and -1s\n",
    "                weights['body.'+str(i)+'.body.0.weight'] = weights['body.'+str(i)+'.body.0.weight'] * rand_filter_weight\n",
    "        elif option == 14:\n",
    "            # Barely noticable difference here\n",
    "            _i = [random.choice(list(range(15))) for _ in range(5)]\n",
    "            for i in _i:\n",
    "                rand_filter_weight = torch.round(torch.rand_like(weights['body.'+str(i)+'.body.0.weight'])) # Random matrix of 1s and 0s\n",
    "                weights['body.'+str(i)+'.body.0.weight'] = weights['body.'+str(i)+'.body.0.weight'] * rand_filter_weight\n",
    "        elif option == 15:\n",
    "            # Negate some entire filters. Definitely a noticable difference\n",
    "            _i = [random.choice(list(range(15))) for _ in range(5)]\n",
    "            for i in _i:\n",
    "                filters_to_be_zeroed = [random.choice(list(range(64))) for _ in range(32)]\n",
    "                weights['body.'+str(i)+'.body.0.weight'][filters_to_be_zeroed] *= -1\n",
    "        elif option == 16:\n",
    "            # Only keep the max filter value in the conv\n",
    "            _ij = [[random.choice(list(range(15))), random.choice([0, 2])] for _ in range(5)]\n",
    "            for i, j in _ij:\n",
    "                w = torch.reshape(weights['body.'+str(i)+'.body.'+str(j)+'.weight'], shape=(64, 64, 9))\n",
    "                res = torch.topk(w, k=1)\n",
    "\n",
    "                w_new = torch.zeros_like(w).scatter(2, res.indices, res.values)\n",
    "                w_new = w_new.reshape(64, 64, 3, 3)\n",
    "                weights['body.'+str(i)+'.body.'+str(j)+'.weight'] = w_new\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    return weights\n",
    "\n",
    "weights = get_weights()\n",
    "\n",
    "net = EDSR()\n",
    "net.load_state_dict(weights)\n",
    "net.cuda()\n",
    "net.eval()\n",
    "\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "class FolderWithPath(ImageNetSubsetDataset):\n",
    "    def __init__(self, root, transform, **kwargs):\n",
    "        new_root = super(FolderWithPath, self).__init__(root, transform=transform)\n",
    "\n",
    "        classes, class_to_idx = find_classes(new_root)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # save_path = '~/data/hendrycks/DistortedImageNet/' + str(self.option) + '/' + self.idx_to_class[target]\n",
    "        save_path = '/home/jtang/Desktop/EDSR/' + self.idx_to_class[target]\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        save_path += path[path.rindex('/'):]\n",
    "\n",
    "        if np.random.uniform() < 0.05:\n",
    "            weights = get_weights()\n",
    "            net.load_state_dict(weights)\n",
    "        with torch.no_grad():\n",
    "            pre_dist = set([random.randint(1, 4) for _ in range(1)])\n",
    "            body_dist = set([random.randint(1, 5)])\n",
    "            img = trnF.to_pil_image(net(255*sample.unsqueeze(0).cuda(), pre_distortions=pre_dist, body_distortions=body_dist).squeeze().to('cpu').clamp(0, 255)/255.)\n",
    "\n",
    "        img.save(save_path)\n",
    "\n",
    "        return 0\n",
    "\n",
    "\n",
    "distorted_dataset = FolderWithPath(\n",
    "    root=\"/home/jtang/Desktop/val\", transform=test_transform)\n",
    "\n",
    "distorted_dataset[0]\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "  distorted_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for _ in tqdm(loader): continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autual data augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1000 classes ['n01440764' 'n01443537' 'n01484850' 'n01491361' 'n01494475' 'n01496331'\n",
      " 'n01498041' 'n01514668' 'n01514859' 'n01518878' 'n01530575' 'n01531178'\n",
      " 'n01532829' 'n01534433' 'n01537544' 'n01558993' 'n01560419' 'n01580077'\n",
      " 'n01582220' 'n01592084' 'n01601694' 'n01608432' 'n01614925' 'n01616318'\n",
      " 'n01622779' 'n01629819' 'n01630670' 'n01631663' 'n01632458' 'n01632777'\n",
      " 'n01641577' 'n01644373' 'n01644900' 'n01664065' 'n01665541' 'n01667114'\n",
      " 'n01667778' 'n01669191' 'n01675722' 'n01677366' 'n01682714' 'n01685808'\n",
      " 'n01687978' 'n01688243' 'n01689811' 'n01692333' 'n01693334' 'n01694178'\n",
      " 'n01695060' 'n01697457' 'n01698640' 'n01704323' 'n01728572' 'n01728920'\n",
      " 'n01729322' 'n01729977' 'n01734418' 'n01735189' 'n01737021' 'n01739381'\n",
      " 'n01740131' 'n01742172' 'n01744401' 'n01748264' 'n01749939' 'n01751748'\n",
      " 'n01753488' 'n01755581' 'n01756291' 'n01768244' 'n01770081' 'n01770393'\n",
      " 'n01773157' 'n01773549' 'n01773797' 'n01774384' 'n01774750' 'n01775062'\n",
      " 'n01776313' 'n01784675' 'n01795545' 'n01796340' 'n01797886' 'n01798484'\n",
      " 'n01806143' 'n01806567' 'n01807496' 'n01817953' 'n01818515' 'n01819313'\n",
      " 'n01820546' 'n01824575' 'n01828970' 'n01829413' 'n01833805' 'n01843065'\n",
      " 'n01843383' 'n01847000' 'n01855032' 'n01855672' 'n01860187' 'n01871265'\n",
      " 'n01872401' 'n01873310' 'n01877812' 'n01882714' 'n01883070' 'n01910747'\n",
      " 'n01914609' 'n01917289' 'n01924916' 'n01930112' 'n01943899' 'n01944390'\n",
      " 'n01945685' 'n01950731' 'n01955084' 'n01968897' 'n01978287' 'n01978455'\n",
      " 'n01980166' 'n01981276' 'n01983481' 'n01984695' 'n01985128' 'n01986214'\n",
      " 'n01990800' 'n02002556' 'n02002724' 'n02006656' 'n02007558' 'n02009229'\n",
      " 'n02009912' 'n02011460' 'n02012849' 'n02013706' 'n02017213' 'n02018207'\n",
      " 'n02018795' 'n02025239' 'n02027492' 'n02028035' 'n02033041' 'n02037110'\n",
      " 'n02051845' 'n02056570' 'n02058221' 'n02066245' 'n02071294' 'n02074367'\n",
      " 'n02077923' 'n02085620' 'n02085782' 'n02085936' 'n02086079' 'n02086240'\n",
      " 'n02086646' 'n02086910' 'n02087046' 'n02087394' 'n02088094' 'n02088238'\n",
      " 'n02088364' 'n02088466' 'n02088632' 'n02089078' 'n02089867' 'n02089973'\n",
      " 'n02090379' 'n02090622' 'n02090721' 'n02091032' 'n02091134' 'n02091244'\n",
      " 'n02091467' 'n02091635' 'n02091831' 'n02092002' 'n02092339' 'n02093256'\n",
      " 'n02093428' 'n02093647' 'n02093754' 'n02093859' 'n02093991' 'n02094114'\n",
      " 'n02094258' 'n02094433' 'n02095314' 'n02095570' 'n02095889' 'n02096051'\n",
      " 'n02096177' 'n02096294' 'n02096437' 'n02096585' 'n02097047' 'n02097130'\n",
      " 'n02097209' 'n02097298' 'n02097474' 'n02097658' 'n02098105' 'n02098286'\n",
      " 'n02098413' 'n02099267' 'n02099429' 'n02099601' 'n02099712' 'n02099849'\n",
      " 'n02100236' 'n02100583' 'n02100735' 'n02100877' 'n02101006' 'n02101388'\n",
      " 'n02101556' 'n02102040' 'n02102177' 'n02102318' 'n02102480' 'n02102973'\n",
      " 'n02104029' 'n02104365' 'n02105056' 'n02105162' 'n02105251' 'n02105412'\n",
      " 'n02105505' 'n02105641' 'n02105855' 'n02106030' 'n02106166' 'n02106382'\n",
      " 'n02106550' 'n02106662' 'n02107142' 'n02107312' 'n02107574' 'n02107683'\n",
      " 'n02107908' 'n02108000' 'n02108089' 'n02108422' 'n02108551' 'n02108915'\n",
      " 'n02109047' 'n02109525' 'n02109961' 'n02110063' 'n02110185' 'n02110341'\n",
      " 'n02110627' 'n02110806' 'n02110958' 'n02111129' 'n02111277' 'n02111500'\n",
      " 'n02111889' 'n02112018' 'n02112137' 'n02112350' 'n02112706' 'n02113023'\n",
      " 'n02113186' 'n02113624' 'n02113712' 'n02113799' 'n02113978' 'n02114367'\n",
      " 'n02114548' 'n02114712' 'n02114855' 'n02115641' 'n02115913' 'n02116738'\n",
      " 'n02117135' 'n02119022' 'n02119789' 'n02120079' 'n02120505' 'n02123045'\n",
      " 'n02123159' 'n02123394' 'n02123597' 'n02124075' 'n02125311' 'n02127052'\n",
      " 'n02128385' 'n02128757' 'n02128925' 'n02129165' 'n02129604' 'n02130308'\n",
      " 'n02132136' 'n02133161' 'n02134084' 'n02134418' 'n02137549' 'n02138441'\n",
      " 'n02165105' 'n02165456' 'n02167151' 'n02168699' 'n02169497' 'n02172182'\n",
      " 'n02174001' 'n02177972' 'n02190166' 'n02206856' 'n02219486' 'n02226429'\n",
      " 'n02229544' 'n02231487' 'n02233338' 'n02236044' 'n02256656' 'n02259212'\n",
      " 'n02264363' 'n02268443' 'n02268853' 'n02276258' 'n02277742' 'n02279972'\n",
      " 'n02280649' 'n02281406' 'n02281787' 'n02317335' 'n02319095' 'n02321529'\n",
      " 'n02325366' 'n02326432' 'n02328150' 'n02342885' 'n02346627' 'n02356798'\n",
      " 'n02361337' 'n02363005' 'n02364673' 'n02389026' 'n02391049' 'n02395406'\n",
      " 'n02396427' 'n02397096' 'n02398521' 'n02403003' 'n02408429' 'n02410509'\n",
      " 'n02412080' 'n02415577' 'n02417914' 'n02422106' 'n02422699' 'n02423022'\n",
      " 'n02437312' 'n02437616' 'n02441942' 'n02442845' 'n02443114' 'n02443484'\n",
      " 'n02444819' 'n02445715' 'n02447366' 'n02454379' 'n02457408' 'n02480495'\n",
      " 'n02480855' 'n02481823' 'n02483362' 'n02483708' 'n02484975' 'n02486261'\n",
      " 'n02486410' 'n02487347' 'n02488291' 'n02488702' 'n02489166' 'n02490219'\n",
      " 'n02492035' 'n02492660' 'n02493509' 'n02493793' 'n02494079' 'n02497673'\n",
      " 'n02500267' 'n02504013' 'n02504458' 'n02509815' 'n02510455' 'n02514041'\n",
      " 'n02526121' 'n02536864' 'n02606052' 'n02607072' 'n02640242' 'n02641379'\n",
      " 'n02643566' 'n02655020' 'n02666196' 'n02667093' 'n02669723' 'n02672831'\n",
      " 'n02676566' 'n02687172' 'n02690373' 'n02692877' 'n02699494' 'n02701002'\n",
      " 'n02704792' 'n02708093' 'n02727426' 'n02730930' 'n02747177' 'n02749479'\n",
      " 'n02769748' 'n02776631' 'n02777292' 'n02782093' 'n02783161' 'n02786058'\n",
      " 'n02787622' 'n02788148' 'n02790996' 'n02791124' 'n02791270' 'n02793495'\n",
      " 'n02794156' 'n02795169' 'n02797295' 'n02799071' 'n02802426' 'n02804414'\n",
      " 'n02804610' 'n02807133' 'n02808304' 'n02808440' 'n02814533' 'n02814860'\n",
      " 'n02815834' 'n02817516' 'n02823428' 'n02823750' 'n02825657' 'n02834397'\n",
      " 'n02835271' 'n02837789' 'n02840245' 'n02841315' 'n02843684' 'n02859443'\n",
      " 'n02860847' 'n02865351' 'n02869837' 'n02870880' 'n02871525' 'n02877765'\n",
      " 'n02879718' 'n02883205' 'n02892201' 'n02892767' 'n02894605' 'n02895154'\n",
      " 'n02906734' 'n02909870' 'n02910353' 'n02916936' 'n02917067' 'n02927161'\n",
      " 'n02930766' 'n02939185' 'n02948072' 'n02950826' 'n02951358' 'n02951585'\n",
      " 'n02963159' 'n02965783' 'n02966193' 'n02966687' 'n02971356' 'n02974003'\n",
      " 'n02977058' 'n02978881' 'n02979186' 'n02980441' 'n02981792' 'n02988304'\n",
      " 'n02992211' 'n02992529' 'n02999410' 'n03000134' 'n03000247' 'n03000684'\n",
      " 'n03014705' 'n03016953' 'n03017168' 'n03018349' 'n03026506' 'n03028079'\n",
      " 'n03032252' 'n03041632' 'n03042490' 'n03045698' 'n03047690' 'n03062245'\n",
      " 'n03063599' 'n03063689' 'n03065424' 'n03075370' 'n03085013' 'n03089624'\n",
      " 'n03095699' 'n03100240' 'n03109150' 'n03110669' 'n03124043' 'n03124170'\n",
      " 'n03125729' 'n03126707' 'n03127747' 'n03127925' 'n03131574' 'n03133878'\n",
      " 'n03134739' 'n03141823' 'n03146219' 'n03160309' 'n03179701' 'n03180011'\n",
      " 'n03187595' 'n03188531' 'n03196217' 'n03197337' 'n03201208' 'n03207743'\n",
      " 'n03207941' 'n03208938' 'n03216828' 'n03218198' 'n03220513' 'n03223299'\n",
      " 'n03240683' 'n03249569' 'n03250847' 'n03255030' 'n03259280' 'n03271574'\n",
      " 'n03272010' 'n03272562' 'n03290653' 'n03291819' 'n03297495' 'n03314780'\n",
      " 'n03325584' 'n03337140' 'n03344393' 'n03345487' 'n03347037' 'n03355925'\n",
      " 'n03372029' 'n03376595' 'n03379051' 'n03384352' 'n03388043' 'n03388183'\n",
      " 'n03388549' 'n03393912' 'n03394916' 'n03400231' 'n03404251' 'n03417042'\n",
      " 'n03424325' 'n03425413' 'n03443371' 'n03444034' 'n03445777' 'n03445924'\n",
      " 'n03447447' 'n03447721' 'n03450230' 'n03452741' 'n03457902' 'n03459775'\n",
      " 'n03461385' 'n03467068' 'n03476684' 'n03476991' 'n03478589' 'n03481172'\n",
      " 'n03482405' 'n03483316' 'n03485407' 'n03485794' 'n03492542' 'n03494278'\n",
      " 'n03495258' 'n03496892' 'n03498962' 'n03527444' 'n03529860' 'n03530642'\n",
      " 'n03532672' 'n03534580' 'n03535780' 'n03538406' 'n03544143' 'n03584254'\n",
      " 'n03584829' 'n03590841' 'n03594734' 'n03594945' 'n03595614' 'n03598930'\n",
      " 'n03599486' 'n03602883' 'n03617480' 'n03623198' 'n03627232' 'n03630383'\n",
      " 'n03633091' 'n03637318' 'n03642806' 'n03649909' 'n03657121' 'n03658185'\n",
      " 'n03661043' 'n03662601' 'n03666591' 'n03670208' 'n03673027' 'n03676483'\n",
      " 'n03680355' 'n03690938' 'n03691459' 'n03692522' 'n03697007' 'n03706229'\n",
      " 'n03709823' 'n03710193' 'n03710637' 'n03710721' 'n03717622' 'n03720891'\n",
      " 'n03721384' 'n03724870' 'n03729826' 'n03733131' 'n03733281' 'n03733805'\n",
      " 'n03742115' 'n03743016' 'n03759954' 'n03761084' 'n03763968' 'n03764736'\n",
      " 'n03769881' 'n03770439' 'n03770679' 'n03773504' 'n03775071' 'n03775546'\n",
      " 'n03776460' 'n03777568' 'n03777754' 'n03781244' 'n03782006' 'n03785016'\n",
      " 'n03786901' 'n03787032' 'n03788195' 'n03788365' 'n03791053' 'n03792782'\n",
      " 'n03792972' 'n03793489' 'n03794056' 'n03796401' 'n03803284' 'n03804744'\n",
      " 'n03814639' 'n03814906' 'n03825788' 'n03832673' 'n03837869' 'n03838899'\n",
      " 'n03840681' 'n03841143' 'n03843555' 'n03854065' 'n03857828' 'n03866082'\n",
      " 'n03868242' 'n03868863' 'n03871628' 'n03873416' 'n03874293' 'n03874599'\n",
      " 'n03876231' 'n03877472' 'n03877845' 'n03884397' 'n03887697' 'n03888257'\n",
      " 'n03888605' 'n03891251' 'n03891332' 'n03895866' 'n03899768' 'n03902125'\n",
      " 'n03903868' 'n03908618' 'n03908714' 'n03916031' 'n03920288' 'n03924679'\n",
      " 'n03929660' 'n03929855' 'n03930313' 'n03930630' 'n03933933' 'n03935335'\n",
      " 'n03937543' 'n03938244' 'n03942813' 'n03944341' 'n03947888' 'n03950228'\n",
      " 'n03954731' 'n03956157' 'n03958227' 'n03961711' 'n03967562' 'n03970156'\n",
      " 'n03976467' 'n03976657' 'n03977966' 'n03980874' 'n03982430' 'n03983396'\n",
      " 'n03991062' 'n03992509' 'n03995372' 'n03998194' 'n04004767' 'n04005630'\n",
      " 'n04008634' 'n04009552' 'n04019541' 'n04023962' 'n04026417' 'n04033901'\n",
      " 'n04033995' 'n04037443' 'n04039381' 'n04040759' 'n04041544' 'n04044716'\n",
      " 'n04049303' 'n04065272' 'n04067472' 'n04069434' 'n04070727' 'n04074963'\n",
      " 'n04081281' 'n04086273' 'n04090263' 'n04099969' 'n04111531' 'n04116512'\n",
      " 'n04118538' 'n04118776' 'n04120489' 'n04125021' 'n04127249' 'n04131690'\n",
      " 'n04133789' 'n04136333' 'n04141076' 'n04141327' 'n04141975' 'n04146614'\n",
      " 'n04147183' 'n04149813' 'n04152593' 'n04153751' 'n04154565' 'n04162706'\n",
      " 'n04179913' 'n04192698' 'n04200800' 'n04201297' 'n04204238' 'n04204347'\n",
      " 'n04208210' 'n04209133' 'n04209239' 'n04228054' 'n04229816' 'n04235860'\n",
      " 'n04238763' 'n04239074' 'n04243546' 'n04251144' 'n04252077' 'n04252225'\n",
      " 'n04254120' 'n04254680' 'n04254777' 'n04258138' 'n04259630' 'n04263257'\n",
      " 'n04264628' 'n04265275' 'n04266014' 'n04270147' 'n04273569' 'n04275548'\n",
      " 'n04277352' 'n04285008' 'n04286575' 'n04296562' 'n04310018' 'n04311004'\n",
      " 'n04311174' 'n04317175' 'n04325704' 'n04326547' 'n04328186' 'n04330267'\n",
      " 'n04332243' 'n04335435' 'n04336792' 'n04344873' 'n04346328' 'n04347754'\n",
      " 'n04350905' 'n04355338' 'n04355933' 'n04356056' 'n04357314' 'n04366367'\n",
      " 'n04367480' 'n04370456' 'n04371430' 'n04371774' 'n04372370' 'n04376876'\n",
      " 'n04380533' 'n04389033' 'n04392985' 'n04398044' 'n04399382' 'n04404412'\n",
      " 'n04409515' 'n04417672' 'n04418357' 'n04423845' 'n04428191' 'n04429376'\n",
      " 'n04435653' 'n04442312' 'n04443257' 'n04447861' 'n04456115' 'n04458633'\n",
      " 'n04461696' 'n04462240' 'n04465501' 'n04467665' 'n04476259' 'n04479046'\n",
      " 'n04482393' 'n04483307' 'n04485082' 'n04486054' 'n04487081' 'n04487394'\n",
      " 'n04493381' 'n04501370' 'n04505470' 'n04507155' 'n04509417' 'n04515003'\n",
      " 'n04517823' 'n04522168' 'n04523525' 'n04525038' 'n04525305' 'n04532106'\n",
      " 'n04532670' 'n04536866' 'n04540053' 'n04542943' 'n04548280' 'n04548362'\n",
      " 'n04550184' 'n04552348' 'n04553703' 'n04554684' 'n04557648' 'n04560804'\n",
      " 'n04562935' 'n04579145' 'n04579432' 'n04584207' 'n04589890' 'n04590129'\n",
      " 'n04591157' 'n04591713' 'n04592741' 'n04596742' 'n04597913' 'n04599235'\n",
      " 'n04604644' 'n04606251' 'n04612504' 'n04613696' 'n06359193' 'n06596364'\n",
      " 'n06785654' 'n06794110' 'n06874185' 'n07248320' 'n07565083' 'n07579787'\n",
      " 'n07583066' 'n07584110' 'n07590611' 'n07613480' 'n07614500' 'n07615774'\n",
      " 'n07684084' 'n07693725' 'n07695742' 'n07697313' 'n07697537' 'n07711569'\n",
      " 'n07714571' 'n07714990' 'n07715103' 'n07716358' 'n07716906' 'n07717410'\n",
      " 'n07717556' 'n07718472' 'n07718747' 'n07720875' 'n07730033' 'n07734744'\n",
      " 'n07742313' 'n07745940' 'n07747607' 'n07749582' 'n07753113' 'n07753275'\n",
      " 'n07753592' 'n07754684' 'n07760859' 'n07768694' 'n07802026' 'n07831146'\n",
      " 'n07836838' 'n07860988' 'n07871810' 'n07873807' 'n07875152' 'n07880968'\n",
      " 'n07892512' 'n07920052' 'n07930864' 'n07932039' 'n09193705' 'n09229709'\n",
      " 'n09246464' 'n09256479' 'n09288635' 'n09332890' 'n09399592' 'n09421951'\n",
      " 'n09428293' 'n09468604' 'n09472597' 'n09835506' 'n10148035' 'n10565667'\n",
      " 'n11879895' 'n11939491' 'n12057211' 'n12144580' 'n12267677' 'n12620546'\n",
      " 'n12768682' 'n12985857' 'n12998815' 'n13037406' 'n13040303' 'n13044778'\n",
      " 'n13052670' 'n13054560' 'n13133613' 'n15075141']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8011/8011 [1:43:18<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#parser = argparse.ArgumentParser(description='Fine-tune')\n",
    "#parser.add_argument('--total-workers', default=None, type=int, required=True)\n",
    "#parser.add_argument('--worker-number', default=None, type=int, required=True) # MUST BE 0-INDEXED\n",
    "#args = parser.parse_args()\n",
    "total_workers = 1\n",
    "worker_number = 0\n",
    "\n",
    "all_classes = [\"n01440764\", \"n01443537\", \"n01484850\", \"n01491361\", \"n01494475\", \"n01496331\", \"n01498041\", \"n01514668\", \"n01514859\", \"n01518878\", \"n01530575\", \"n01531178\", \"n01532829\", \"n01534433\", \"n01537544\", \"n01558993\", \"n01560419\", \"n01580077\", \"n01582220\", \"n01592084\", \"n01601694\", \"n01608432\", \"n01614925\", \"n01616318\", \"n01622779\", \"n01629819\", \"n01630670\", \"n01631663\", \"n01632458\", \"n01632777\", \"n01641577\", \"n01644373\", \"n01644900\", \"n01664065\", \"n01665541\", \"n01667114\", \"n01667778\", \"n01669191\", \"n01675722\", \"n01677366\", \"n01682714\", \"n01685808\", \"n01687978\", \"n01688243\", \"n01689811\", \"n01692333\", \"n01693334\", \"n01694178\", \"n01695060\", \"n01697457\", \"n01698640\", \"n01704323\", \"n01728572\", \"n01728920\", \"n01729322\", \"n01729977\", \"n01734418\", \"n01735189\", \"n01737021\", \"n01739381\", \"n01740131\", \"n01742172\", \"n01744401\", \"n01748264\", \"n01749939\", \"n01751748\", \"n01753488\", \"n01755581\", \"n01756291\", \"n01768244\", \"n01770081\", \"n01770393\", \"n01773157\", \"n01773549\", \"n01773797\", \"n01774384\", \"n01774750\", \"n01775062\", \"n01776313\", \"n01784675\", \"n01795545\", \"n01796340\", \"n01797886\", \"n01798484\", \"n01806143\", \"n01806567\", \"n01807496\", \"n01817953\", \"n01818515\", \"n01819313\", \"n01820546\", \"n01824575\", \"n01828970\", \"n01829413\", \"n01833805\", \"n01843065\", \"n01843383\", \"n01847000\", \"n01855032\", \"n01855672\", \"n01860187\", \"n01871265\", \"n01872401\", \"n01873310\", \"n01877812\", \"n01882714\", \"n01883070\", \"n01910747\", \"n01914609\", \"n01917289\", \"n01924916\", \"n01930112\", \"n01943899\", \"n01944390\", \"n01945685\", \"n01950731\", \"n01955084\", \"n01968897\", \"n01978287\", \"n01978455\", \"n01980166\", \"n01981276\", \"n01983481\", \"n01984695\", \"n01985128\", \"n01986214\", \"n01990800\", \"n02002556\", \"n02002724\", \"n02006656\", \"n02007558\", \"n02009229\", \"n02009912\", \"n02011460\", \"n02012849\", \"n02013706\", \"n02017213\", \"n02018207\", \"n02018795\", \"n02025239\", \"n02027492\", \"n02028035\", \"n02033041\", \"n02037110\", \"n02051845\", \"n02056570\", \"n02058221\", \"n02066245\", \"n02071294\", \"n02074367\", \"n02077923\", \"n02085620\", \"n02085782\", \"n02085936\", \"n02086079\", \"n02086240\", \"n02086646\", \"n02086910\", \"n02087046\", \"n02087394\", \"n02088094\", \"n02088238\", \"n02088364\", \"n02088466\", \"n02088632\", \"n02089078\", \"n02089867\", \"n02089973\", \"n02090379\", \"n02090622\", \"n02090721\", \"n02091032\", \"n02091134\", \"n02091244\", \"n02091467\", \"n02091635\", \"n02091831\", \"n02092002\", \"n02092339\", \"n02093256\", \"n02093428\", \"n02093647\", \"n02093754\", \"n02093859\", \"n02093991\", \"n02094114\", \"n02094258\", \"n02094433\", \"n02095314\", \"n02095570\", \"n02095889\", \"n02096051\", \"n02096177\", \"n02096294\", \"n02096437\", \"n02096585\", \"n02097047\", \"n02097130\", \"n02097209\", \"n02097298\", \"n02097474\", \"n02097658\", \"n02098105\", \"n02098286\", \"n02098413\", \"n02099267\", \"n02099429\", \"n02099601\", \"n02099712\", \"n02099849\", \"n02100236\", \"n02100583\", \"n02100735\", \"n02100877\", \"n02101006\", \"n02101388\", \"n02101556\", \"n02102040\", \"n02102177\", \"n02102318\", \"n02102480\", \"n02102973\", \"n02104029\", \"n02104365\", \"n02105056\", \"n02105162\", \"n02105251\", \"n02105412\", \"n02105505\", \"n02105641\", \"n02105855\", \"n02106030\", \"n02106166\", \"n02106382\", \"n02106550\", \"n02106662\", \"n02107142\", \"n02107312\", \"n02107574\", \"n02107683\", \"n02107908\", \"n02108000\", \"n02108089\", \"n02108422\", \"n02108551\", \"n02108915\", \"n02109047\", \"n02109525\", \"n02109961\", \"n02110063\", \"n02110185\", \"n02110341\", \"n02110627\", \"n02110806\", \"n02110958\", \"n02111129\", \"n02111277\", \"n02111500\", \"n02111889\", \"n02112018\", \"n02112137\", \"n02112350\", \"n02112706\", \"n02113023\", \"n02113186\", \"n02113624\", \"n02113712\", \"n02113799\", \"n02113978\", \"n02114367\", \"n02114548\", \"n02114712\", \"n02114855\", \"n02115641\", \"n02115913\", \"n02116738\", \"n02117135\", \"n02119022\", \"n02119789\", \"n02120079\", \"n02120505\", \"n02123045\", \"n02123159\", \"n02123394\", \"n02123597\", \"n02124075\", \"n02125311\", \"n02127052\", \"n02128385\", \"n02128757\", \"n02128925\", \"n02129165\", \"n02129604\", \"n02130308\", \"n02132136\", \"n02133161\", \"n02134084\", \"n02134418\", \"n02137549\", \"n02138441\", \"n02165105\", \"n02165456\", \"n02167151\", \"n02168699\", \"n02169497\", \"n02172182\", \"n02174001\", \"n02177972\", \"n02190166\", \"n02206856\", \"n02219486\", \"n02226429\", \"n02229544\", \"n02231487\", \"n02233338\", \"n02236044\", \"n02256656\", \"n02259212\", \"n02264363\", \"n02268443\", \"n02268853\", \"n02276258\", \"n02277742\", \"n02279972\", \"n02280649\", \"n02281406\", \"n02281787\", \"n02317335\", \"n02319095\", \"n02321529\", \"n02325366\", \"n02326432\", \"n02328150\", \"n02342885\", \"n02346627\", \"n02356798\", \"n02361337\", \"n02363005\", \"n02364673\", \"n02389026\", \"n02391049\", \"n02395406\", \"n02396427\", \"n02397096\", \"n02398521\", \"n02403003\", \"n02408429\", \"n02410509\", \"n02412080\", \"n02415577\", \"n02417914\", \"n02422106\", \"n02422699\", \"n02423022\", \"n02437312\", \"n02437616\", \"n02441942\", \"n02442845\", \"n02443114\", \"n02443484\", \"n02444819\", \"n02445715\", \"n02447366\", \"n02454379\", \"n02457408\", \"n02480495\", \"n02480855\", \"n02481823\", \"n02483362\", \"n02483708\", \"n02484975\", \"n02486261\", \"n02486410\", \"n02487347\", \"n02488291\", \"n02488702\", \"n02489166\", \"n02490219\", \"n02492035\", \"n02492660\", \"n02493509\", \"n02493793\", \"n02494079\", \"n02497673\", \"n02500267\", \"n02504013\", \"n02504458\", \"n02509815\", \"n02510455\", \"n02514041\", \"n02526121\", \"n02536864\", \"n02606052\", \"n02607072\", \"n02640242\", \"n02641379\", \"n02643566\", \"n02655020\", \"n02666196\", \"n02667093\", \"n02669723\", \"n02672831\", \"n02676566\", \"n02687172\", \"n02690373\", \"n02692877\", \"n02699494\", \"n02701002\", \"n02704792\", \"n02708093\", \"n02727426\", \"n02730930\", \"n02747177\", \"n02749479\", \"n02769748\", \"n02776631\", \"n02777292\", \"n02782093\", \"n02783161\", \"n02786058\", \"n02787622\", \"n02788148\", \"n02790996\", \"n02791124\", \"n02791270\", \"n02793495\", \"n02794156\", \"n02795169\", \"n02797295\", \"n02799071\", \"n02802426\", \"n02804414\", \"n02804610\", \"n02807133\", \"n02808304\", \"n02808440\", \"n02814533\", \"n02814860\", \"n02815834\", \"n02817516\", \"n02823428\", \"n02823750\", \"n02825657\", \"n02834397\", \"n02835271\", \"n02837789\", \"n02840245\", \"n02841315\", \"n02843684\", \"n02859443\", \"n02860847\", \"n02865351\", \"n02869837\", \"n02870880\", \"n02871525\", \"n02877765\", \"n02879718\", \"n02883205\", \"n02892201\", \"n02892767\", \"n02894605\", \"n02895154\", \"n02906734\", \"n02909870\", \"n02910353\", \"n02916936\", \"n02917067\", \"n02927161\", \"n02930766\", \"n02939185\", \"n02948072\", \"n02950826\", \"n02951358\", \"n02951585\", \"n02963159\", \"n02965783\", \"n02966193\", \"n02966687\", \"n02971356\", \"n02974003\", \"n02977058\", \"n02978881\", \"n02979186\", \"n02980441\", \"n02981792\", \"n02988304\", \"n02992211\", \"n02992529\", \"n02999410\", \"n03000134\", \"n03000247\", \"n03000684\", \"n03014705\", \"n03016953\", \"n03017168\", \"n03018349\", \"n03026506\", \"n03028079\", \"n03032252\", \"n03041632\", \"n03042490\", \"n03045698\", \"n03047690\", \"n03062245\", \"n03063599\", \"n03063689\", \"n03065424\", \"n03075370\", \"n03085013\", \"n03089624\", \"n03095699\", \"n03100240\", \"n03109150\", \"n03110669\", \"n03124043\", \"n03124170\", \"n03125729\", \"n03126707\", \"n03127747\", \"n03127925\", \"n03131574\", \"n03133878\", \"n03134739\", \"n03141823\", \"n03146219\", \"n03160309\", \"n03179701\", \"n03180011\", \"n03187595\", \"n03188531\", \"n03196217\", \"n03197337\", \"n03201208\", \"n03207743\", \"n03207941\", \"n03208938\", \"n03216828\", \"n03218198\", \"n03220513\", \"n03223299\", \"n03240683\", \"n03249569\", \"n03250847\", \"n03255030\", \"n03259280\", \"n03271574\", \"n03272010\", \"n03272562\", \"n03290653\", \"n03291819\", \"n03297495\", \"n03314780\", \"n03325584\", \"n03337140\", \"n03344393\", \"n03345487\", \"n03347037\", \"n03355925\", \"n03372029\", \"n03376595\", \"n03379051\", \"n03384352\", \"n03388043\", \"n03388183\", \"n03388549\", \"n03393912\", \"n03394916\", \"n03400231\", \"n03404251\", \"n03417042\", \"n03424325\", \"n03425413\", \"n03443371\", \"n03444034\", \"n03445777\", \"n03445924\", \"n03447447\", \"n03447721\", \"n03450230\", \"n03452741\", \"n03457902\", \"n03459775\", \"n03461385\", \"n03467068\", \"n03476684\", \"n03476991\", \"n03478589\", \"n03481172\", \"n03482405\", \"n03483316\", \"n03485407\", \"n03485794\", \"n03492542\", \"n03494278\", \"n03495258\", \"n03496892\", \"n03498962\", \"n03527444\", \"n03529860\", \"n03530642\", \"n03532672\", \"n03534580\", \"n03535780\", \"n03538406\", \"n03544143\", \"n03584254\", \"n03584829\", \"n03590841\", \"n03594734\", \"n03594945\", \"n03595614\", \"n03598930\", \"n03599486\", \"n03602883\", \"n03617480\", \"n03623198\", \"n03627232\", \"n03630383\", \"n03633091\", \"n03637318\", \"n03642806\", \"n03649909\", \"n03657121\", \"n03658185\", \"n03661043\", \"n03662601\", \"n03666591\", \"n03670208\", \"n03673027\", \"n03676483\", \"n03680355\", \"n03690938\", \"n03691459\", \"n03692522\", \"n03697007\", \"n03706229\", \"n03709823\", \"n03710193\", \"n03710637\", \"n03710721\", \"n03717622\", \"n03720891\", \"n03721384\", \"n03724870\", \"n03729826\", \"n03733131\", \"n03733281\", \"n03733805\", \"n03742115\", \"n03743016\", \"n03759954\", \"n03761084\", \"n03763968\", \"n03764736\", \"n03769881\", \"n03770439\", \"n03770679\", \"n03773504\", \"n03775071\", \"n03775546\", \"n03776460\", \"n03777568\", \"n03777754\", \"n03781244\", \"n03782006\", \"n03785016\", \"n03786901\", \"n03787032\", \"n03788195\", \"n03788365\", \"n03791053\", \"n03792782\", \"n03792972\", \"n03793489\", \"n03794056\", \"n03796401\", \"n03803284\", \"n03804744\", \"n03814639\", \"n03814906\", \"n03825788\", \"n03832673\", \"n03837869\", \"n03838899\", \"n03840681\", \"n03841143\", \"n03843555\", \"n03854065\", \"n03857828\", \"n03866082\", \"n03868242\", \"n03868863\", \"n03871628\", \"n03873416\", \"n03874293\", \"n03874599\", \"n03876231\", \"n03877472\", \"n03877845\", \"n03884397\", \"n03887697\", \"n03888257\", \"n03888605\", \"n03891251\", \"n03891332\", \"n03895866\", \"n03899768\", \"n03902125\", \"n03903868\", \"n03908618\", \"n03908714\", \"n03916031\", \"n03920288\", \"n03924679\", \"n03929660\", \"n03929855\", \"n03930313\", \"n03930630\", \"n03933933\", \"n03935335\", \"n03937543\", \"n03938244\", \"n03942813\", \"n03944341\", \"n03947888\", \"n03950228\", \"n03954731\", \"n03956157\", \"n03958227\", \"n03961711\", \"n03967562\", \"n03970156\", \"n03976467\", \"n03976657\", \"n03977966\", \"n03980874\", \"n03982430\", \"n03983396\", \"n03991062\", \"n03992509\", \"n03995372\", \"n03998194\", \"n04004767\", \"n04005630\", \"n04008634\", \"n04009552\", \"n04019541\", \"n04023962\", \"n04026417\", \"n04033901\", \"n04033995\", \"n04037443\", \"n04039381\", \"n04040759\", \"n04041544\", \"n04044716\", \"n04049303\", \"n04065272\", \"n04067472\", \"n04069434\", \"n04070727\", \"n04074963\", \"n04081281\", \"n04086273\", \"n04090263\", \"n04099969\", \"n04111531\", \"n04116512\", \"n04118538\", \"n04118776\", \"n04120489\", \"n04125021\", \"n04127249\", \"n04131690\", \"n04133789\", \"n04136333\", \"n04141076\", \"n04141327\", \"n04141975\", \"n04146614\", \"n04147183\", \"n04149813\", \"n04152593\", \"n04153751\", \"n04154565\", \"n04162706\", \"n04179913\", \"n04192698\", \"n04200800\", \"n04201297\", \"n04204238\", \"n04204347\", \"n04208210\", \"n04209133\", \"n04209239\", \"n04228054\", \"n04229816\", \"n04235860\", \"n04238763\", \"n04239074\", \"n04243546\", \"n04251144\", \"n04252077\", \"n04252225\", \"n04254120\", \"n04254680\", \"n04254777\", \"n04258138\", \"n04259630\", \"n04263257\", \"n04264628\", \"n04265275\", \"n04266014\", \"n04270147\", \"n04273569\", \"n04275548\", \"n04277352\", \"n04285008\", \"n04286575\", \"n04296562\", \"n04310018\", \"n04311004\", \"n04311174\", \"n04317175\", \"n04325704\", \"n04326547\", \"n04328186\", \"n04330267\", \"n04332243\", \"n04335435\", \"n04336792\", \"n04344873\", \"n04346328\", \"n04347754\", \"n04350905\", \"n04355338\", \"n04355933\", \"n04356056\", \"n04357314\", \"n04366367\", \"n04367480\", \"n04370456\", \"n04371430\", \"n04371774\", \"n04372370\", \"n04376876\", \"n04380533\", \"n04389033\", \"n04392985\", \"n04398044\", \"n04399382\", \"n04404412\", \"n04409515\", \"n04417672\", \"n04418357\", \"n04423845\", \"n04428191\", \"n04429376\", \"n04435653\", \"n04442312\", \"n04443257\", \"n04447861\", \"n04456115\", \"n04458633\", \"n04461696\", \"n04462240\", \"n04465501\", \"n04467665\", \"n04476259\", \"n04479046\", \"n04482393\", \"n04483307\", \"n04485082\", \"n04486054\", \"n04487081\", \"n04487394\", \"n04493381\", \"n04501370\", \"n04505470\", \"n04507155\", \"n04509417\", \"n04515003\", \"n04517823\", \"n04522168\", \"n04523525\", \"n04525038\", \"n04525305\", \"n04532106\", \"n04532670\", \"n04536866\", \"n04540053\", \"n04542943\", \"n04548280\", \"n04548362\", \"n04550184\", \"n04552348\", \"n04553703\", \"n04554684\", \"n04557648\", \"n04560804\", \"n04562935\", \"n04579145\", \"n04579432\", \"n04584207\", \"n04589890\", \"n04590129\", \"n04591157\", \"n04591713\", \"n04592741\", \"n04596742\", \"n04597913\", \"n04599235\", \"n04604644\", \"n04606251\", \"n04612504\", \"n04613696\", \"n06359193\", \"n06596364\", \"n06785654\", \"n06794110\", \"n06874185\", \"n07248320\", \"n07565083\", \"n07579787\", \"n07583066\", \"n07584110\", \"n07590611\", \"n07613480\", \"n07614500\", \"n07615774\", \"n07684084\", \"n07693725\", \"n07695742\", \"n07697313\", \"n07697537\", \"n07711569\", \"n07714571\", \"n07714990\", \"n07715103\", \"n07716358\", \"n07716906\", \"n07717410\", \"n07717556\", \"n07718472\", \"n07718747\", \"n07720875\", \"n07730033\", \"n07734744\", \"n07742313\", \"n07745940\", \"n07747607\", \"n07749582\", \"n07753113\", \"n07753275\", \"n07753592\", \"n07754684\", \"n07760859\", \"n07768694\", \"n07802026\", \"n07831146\", \"n07836838\", \"n07860988\", \"n07871810\", \"n07873807\", \"n07875152\", \"n07880968\", \"n07892512\", \"n07920052\", \"n07930864\", \"n07932039\", \"n09193705\", \"n09229709\", \"n09246464\", \"n09256479\", \"n09288635\", \"n09332890\", \"n09399592\", \"n09421951\", \"n09428293\", \"n09468604\", \"n09472597\", \"n09835506\", \"n10148035\", \"n10565667\", \"n11879895\", \"n11939491\", \"n12057211\", \"n12144580\", \"n12267677\", \"n12620546\", \"n12768682\", \"n12985857\", \"n12998815\", \"n13037406\", \"n13040303\", \"n13044778\", \"n13052670\", \"n13054560\", \"n13133613\", \"n15075141\"]\n",
    "all_classes.sort()\n",
    "assert len(all_classes) == 1000\n",
    "\n",
    "# Subset for this worker\n",
    "classes_chosen = np.array_split(all_classes, total_workers)[worker_number]\n",
    "\n",
    "class ImageNetSubsetDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class to take a specified subset of some larger dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, root, *args, **kwargs):\n",
    "\n",
    "        print(\"Using {0} classes {1}\".format(len(classes_chosen), classes_chosen))\n",
    "\n",
    "        self.new_root = tempfile.mkdtemp()\n",
    "        for _class in classes_chosen:\n",
    "            orig_dir = os.path.join(root, _class)\n",
    "            #assert os.path.isdir(orig_dir)\n",
    "\n",
    "            os.symlink(orig_dir, os.path.join(self.new_root, _class))\n",
    "\n",
    "        super().__init__(self.new_root, *args, **kwargs)\n",
    "\n",
    "        return self.new_root\n",
    "\n",
    "    def __del__(self):\n",
    "        # Clean up\n",
    "        shutil.rmtree(self.new_root)\n",
    "\n",
    "\n",
    "# test_transform = lambda x: trnF.to_tensor(trnF.resize(x, 128))\n",
    "test_transform = trn.Compose([trn.Resize(128), trn.ToTensor()])\n",
    "\n",
    "\n",
    "def make_model(args, parent=False):\n",
    "    return EDSR(args)\n",
    "\n",
    "class EDSR(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_resblocks=16, n_feats=64, scale=4, res_scale=1, rgb_range=255, n_colors=3,\n",
    "                 conv=common.default_conv):\n",
    "        super(EDSR, self).__init__()\n",
    "\n",
    "        kernel_size = 3\n",
    "        act = nn.ReLU(True)\n",
    "        url_name = 'r{}f{}x{}'.format(n_resblocks, n_feats, scale)\n",
    "        self.sub_mean = common.MeanShift(rgb_range)\n",
    "        self.add_mean = common.MeanShift(rgb_range, sign=1)\n",
    "\n",
    "        # define head module\n",
    "        m_head = [conv(n_colors, n_feats, kernel_size)]\n",
    "\n",
    "        # define body module\n",
    "        m_body = [\n",
    "            common.ResBlock(\n",
    "                conv, n_feats, kernel_size, act=act, res_scale=res_scale\n",
    "            ) for _ in range(n_resblocks)\n",
    "        ]\n",
    "        m_body.append(conv(n_feats, n_feats, kernel_size))\n",
    "\n",
    "        # define tail module\n",
    "        m_tail = [\n",
    "            common.Upsampler(conv, scale, n_feats, act=False),\n",
    "            conv(n_feats, n_colors, kernel_size)\n",
    "        ]\n",
    "\n",
    "        self.head = nn.Sequential(*m_head)\n",
    "        self.body = nn.Sequential(*m_body)\n",
    "        self.tail = nn.Sequential(*m_tail)\n",
    "\n",
    "    def forward(self, x, pre_distortions={None}, body_distortions={None}):\n",
    "        # print(\"Using FF pre distortions = \", pre_distortions)\n",
    "        # print(\"Using FF body distortions = \", body_distortions)\n",
    "        x = self.sub_mean(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        ######################################################################\n",
    "        # PRE - DISTORTIONS\n",
    "        ######################################################################\n",
    "\n",
    "        if 1 in pre_distortions:\n",
    "            for _ in range(5):\n",
    "                c1, c2 = random.randint(0, 63), random.randint(0, 63)\n",
    "                x[:, c1], x[:, c2] = x[:, c2], x[:, c1]\n",
    "\n",
    "        if 2 in pre_distortions:\n",
    "            rand_filter_weight = torch.round(torch.rand_like(x) + 0.45) # Random matrix of 1s and 0s\n",
    "            x = x * rand_filter_weight\n",
    "\n",
    "        if 3 in pre_distortions:\n",
    "            rand_filter_weight = (torch.round(torch.rand_like(x) + 0.475) * 2) - 1 # Random matrix of 1s and -1s\n",
    "            x = x * rand_filter_weight\n",
    "\n",
    "        ######################################################################\n",
    "        # BODY - DISTORTIONS\n",
    "        ######################################################################\n",
    "\n",
    "        if 1 in body_distortions:\n",
    "            res = self.body[:5](x)\n",
    "            res = -res\n",
    "            res = self.body[5:](res)\n",
    "        elif 2 in body_distortions:\n",
    "            if random.randint(0, 2) == 1:\n",
    "                act = F.relu\n",
    "            else:\n",
    "                act = F.gelu\n",
    "            res = self.body[:5](x)\n",
    "            res = act(res)\n",
    "            res = self.body[5:](res)\n",
    "        elif 3 in body_distortions:\n",
    "            if random.randint(0, 2) == 1:\n",
    "                axes = [1, 2]\n",
    "            else:\n",
    "                axes = [1, 3]\n",
    "            res = self.body[:5](x)\n",
    "            res = torch.flip(res, axes)\n",
    "            res = self.body[5:](res)\n",
    "        elif 4 in body_distortions:\n",
    "            to_skip = set([random.randint(2, 16) for _ in range(3)])\n",
    "            for i in range(len(self.body)):\n",
    "                if i not in to_skip:\n",
    "                    res = self.body[i](x)\n",
    "        else:\n",
    "            res = self.body(x)\n",
    "\n",
    "        res += x\n",
    "\n",
    "        x = self.tail(res)\n",
    "        x = self.add_mean(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_weights():\n",
    "    weights = torch.load('./EDSR_Weights/edsr_baseline_x4.pt')\n",
    "\n",
    "    random_sample_list = np.random.randint(0,17, size=3)\n",
    "    for option in list(random_sample_list):\n",
    "        if option == 0:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = torch.flip(weights['body.'+str(i)+'.body.0.weight'], (0,))\n",
    "            weights['body.'+str(i)+'.body.0.bias'] = torch.flip(weights['body.'+str(i)+'.body.0.bias'], (0,))\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = torch.flip(weights['body.'+str(i)+'.body.2.weight'], (0,))\n",
    "            weights['body.'+str(i)+'.body.2.bias'] = torch.flip(weights['body.'+str(i)+'.body.2.bias'], (0,))\n",
    "        elif option == 1:\n",
    "            i = np.random.choice(np.arange(1,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = -weights['body.'+str(i)+'.body.0.weight']\n",
    "            weights['body.'+str(i)+'.body.0.bias'] = -weights['body.'+str(i)+'.body.0.bias']\n",
    "        elif option == 2:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = 0 * weights['body.'+str(i)+'.body.0.weight']\n",
    "            weights['body.'+str(i)+'.body.0.bias'] = 0*weights['body.'+str(i)+'.body.0.bias']\n",
    "        elif option == 3:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = -gelu(weights['body.'+str(i)+'.body.0.weight'])\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = -gelu(weights['body.'+str(i)+'.body.2.weight'])\n",
    "        elif option == 4:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = weights['body.'+str(i)+'.body.0.weight'] *\\\n",
    "            torch.Tensor([[0, 1, 0],[1, -4., 1], [0, 1, 0]]).view(1,1,3,3).cuda()\n",
    "        elif option == 5:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = weights['body.'+str(i)+'.body.0.weight'] *\\\n",
    "            torch.Tensor([[-1, -1, -1],[-1, 8., -1], [-1, -1, -1]]).view(1,1,3,3).cuda()\n",
    "        elif option == 6:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = weights['body.'+str(i)+'.body.2.weight'] *\\\n",
    "            (1 + 2 * np.float32(np.random.uniform()) * (2*torch.rand_like(weights['body.'+str(i)+'.body.2.weight']-1)))\n",
    "        elif option == 7:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = torch.flip(weights['body.'+str(i)+'.body.0.weight'], (-1,))\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = -1 * weights['body.'+str(i)+'.body.2.weight']\n",
    "        elif option == 8:\n",
    "            i = np.random.choice(np.arange(1,13,4))\n",
    "            z = torch.zeros_like(weights['body.'+str(i)+'.body.0.weight'])\n",
    "            for j in range(z.size(0)):\n",
    "                shift_x, shift_y = np.random.randint(3, size=(2,))\n",
    "                z[:,j,shift_x,shift_y] = np.random.choice([1.,-1.])\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = conv2d(weights['body.'+str(i)+'.body.0.weight'], z, padding=1)\n",
    "        elif option == 9:\n",
    "            i = np.random.choice(np.arange(0,10,3))\n",
    "            z = (2*torch.rand_like(weights['body.'+str(i)+'.body.0.weight'])*np.float32(np.random.uniform()) - 1)/6.\n",
    "            weights['body.'+str(i)+'.body.0.weight'] = conv2d(weights['body.'+str(i)+'.body.0.weight'], z, padding=1)\n",
    "        elif option == 10:\n",
    "            i = np.random.choice(np.arange(1,12,4))\n",
    "            z = torch.FloatTensor(np.random.dirichlet([0.1] * 9, (64,64))).view(64,64,3,3).cuda() # 2.weight\n",
    "            weights['body.'+str(i)+'.body.2.weight'] = conv2d(weights['body.'+str(i)+'.body.2.weight'], z, padding=1)\n",
    "        elif option == 11: ############ Start Saurav's changes ############\n",
    "            i = random.choice(list(range(15)))\n",
    "            noise = (torch.rand_like(weights['body.'+str(i)+'.body.2.weight']) - 0.5) * 1.0\n",
    "            weights['body.'+str(i)+'.body.2.weight'] += noise\n",
    "        elif option == 12:\n",
    "            _ij = [[random.choice(list(range(15))), random.choice([0, 2])] for _ in range(5)]\n",
    "            for i, j in _ij:\n",
    "                _k = random.randint(1, 3)\n",
    "                if random.randint(0, 1) == 0:\n",
    "                    _dims = (2,3)\n",
    "                else:\n",
    "                    _dims = (0,1)\n",
    "                weights['body.'+str(i)+'.body.'+str(j)+'.weight'] = torch.rot90(weights['body.'+str(i)+'.body.'+str(j)+'.weight'], k=_k, dims=_dims)\n",
    "        elif option == 13:\n",
    "            _i = [random.choice(list(range(15))) for _ in range(5)]\n",
    "            for i in _i:\n",
    "                rand_filter_weight = torch.round(torch.rand_like(weights['body.'+str(i)+'.body.0.weight'])) * 2 - 1 # Random matrix of 1s and -1s\n",
    "                weights['body.'+str(i)+'.body.0.weight'] = weights['body.'+str(i)+'.body.0.weight'] * rand_filter_weight\n",
    "        elif option == 14:\n",
    "            # Barely noticable difference here\n",
    "            _i = [random.choice(list(range(15))) for _ in range(5)]\n",
    "            for i in _i:\n",
    "                rand_filter_weight = torch.round(torch.rand_like(weights['body.'+str(i)+'.body.0.weight'])) # Random matrix of 1s and 0s\n",
    "                weights['body.'+str(i)+'.body.0.weight'] = weights['body.'+str(i)+'.body.0.weight'] * rand_filter_weight\n",
    "        elif option == 15:\n",
    "            # Negate some entire filters. Definitely a noticable difference\n",
    "            _i = [random.choice(list(range(15))) for _ in range(5)]\n",
    "            for i in _i:\n",
    "                filters_to_be_zeroed = [random.choice(list(range(64))) for _ in range(32)]\n",
    "                weights['body.'+str(i)+'.body.0.weight'][filters_to_be_zeroed] *= -1\n",
    "        elif option == 16:\n",
    "            # Only keep the max filter value in the conv\n",
    "            _ij = [[random.choice(list(range(15))), random.choice([0, 2])] for _ in range(5)]\n",
    "            for i, j in _ij:\n",
    "                w = torch.reshape(weights['body.'+str(i)+'.body.'+str(j)+'.weight'], shape=(64, 64, 9))\n",
    "                res = torch.topk(w, k=1)\n",
    "\n",
    "                w_new = torch.zeros_like(w).scatter(2, res.indices, res.values)\n",
    "                w_new = w_new.reshape(64, 64, 3, 3)\n",
    "                weights['body.'+str(i)+'.body.'+str(j)+'.weight'] = w_new\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    return weights\n",
    "\n",
    "weights = get_weights()\n",
    "\n",
    "net = EDSR()\n",
    "net.load_state_dict(weights)\n",
    "net.cuda()\n",
    "net.eval()\n",
    "\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "class FolderWithPath(ImageNetSubsetDataset):\n",
    "    def __init__(self, root, transform, **kwargs):\n",
    "        new_root = super(FolderWithPath, self).__init__(root, transform=transform)\n",
    "\n",
    "        classes, class_to_idx = find_classes(new_root)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        # save_path = '~/data/hendrycks/DistortedImageNet/' + str(self.option) + '/' + self.idx_to_class[target]\n",
    "        save_path = '/data/imagenet_augment_EDSR/' + self.idx_to_class[target]\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        save_path += path[path.rindex('/'):]\n",
    "\n",
    "        if np.random.uniform() < 0.05:\n",
    "            weights = get_weights()\n",
    "            net.load_state_dict(weights)\n",
    "        with torch.no_grad():\n",
    "            pre_dist = set([random.randint(1, 4) for _ in range(1)])\n",
    "            body_dist = set([random.randint(1, 5)])\n",
    "            img = trnF.to_pil_image(net(255*sample.unsqueeze(0).cuda(), pre_distortions=pre_dist, body_distortions=body_dist).squeeze().to('cpu').clamp(0, 255)/255.)\n",
    "\n",
    "        img.save(save_path)\n",
    "\n",
    "        return 0\n",
    "\n",
    "\n",
    "distorted_dataset = FolderWithPath(\n",
    "    root=\"/data/imagenet/ILSVRC/Data/CLS-LOC/real_train\", transform=test_transform)\n",
    "\n",
    "distorted_dataset[0]\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "  distorted_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for _ in tqdm(loader): continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms as trn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.functional as trnF \n",
    "from torch.nn.functional import gelu, conv2d\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from CAE_Model.cae_32x32x32_zero_pad_bin import CAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1200 [00:00<03:00,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1000 classes ['n01440764' 'n01443537' 'n01484850' 'n01491361' 'n01494475' 'n01496331'\n",
      " 'n01498041' 'n01514668' 'n01514859' 'n01518878' 'n01530575' 'n01531178'\n",
      " 'n01532829' 'n01534433' 'n01537544' 'n01558993' 'n01560419' 'n01580077'\n",
      " 'n01582220' 'n01592084' 'n01601694' 'n01608432' 'n01614925' 'n01616318'\n",
      " 'n01622779' 'n01629819' 'n01630670' 'n01631663' 'n01632458' 'n01632777'\n",
      " 'n01641577' 'n01644373' 'n01644900' 'n01664065' 'n01665541' 'n01667114'\n",
      " 'n01667778' 'n01669191' 'n01675722' 'n01677366' 'n01682714' 'n01685808'\n",
      " 'n01687978' 'n01688243' 'n01689811' 'n01692333' 'n01693334' 'n01694178'\n",
      " 'n01695060' 'n01697457' 'n01698640' 'n01704323' 'n01728572' 'n01728920'\n",
      " 'n01729322' 'n01729977' 'n01734418' 'n01735189' 'n01737021' 'n01739381'\n",
      " 'n01740131' 'n01742172' 'n01744401' 'n01748264' 'n01749939' 'n01751748'\n",
      " 'n01753488' 'n01755581' 'n01756291' 'n01768244' 'n01770081' 'n01770393'\n",
      " 'n01773157' 'n01773549' 'n01773797' 'n01774384' 'n01774750' 'n01775062'\n",
      " 'n01776313' 'n01784675' 'n01795545' 'n01796340' 'n01797886' 'n01798484'\n",
      " 'n01806143' 'n01806567' 'n01807496' 'n01817953' 'n01818515' 'n01819313'\n",
      " 'n01820546' 'n01824575' 'n01828970' 'n01829413' 'n01833805' 'n01843065'\n",
      " 'n01843383' 'n01847000' 'n01855032' 'n01855672' 'n01860187' 'n01871265'\n",
      " 'n01872401' 'n01873310' 'n01877812' 'n01882714' 'n01883070' 'n01910747'\n",
      " 'n01914609' 'n01917289' 'n01924916' 'n01930112' 'n01943899' 'n01944390'\n",
      " 'n01945685' 'n01950731' 'n01955084' 'n01968897' 'n01978287' 'n01978455'\n",
      " 'n01980166' 'n01981276' 'n01983481' 'n01984695' 'n01985128' 'n01986214'\n",
      " 'n01990800' 'n02002556' 'n02002724' 'n02006656' 'n02007558' 'n02009229'\n",
      " 'n02009912' 'n02011460' 'n02012849' 'n02013706' 'n02017213' 'n02018207'\n",
      " 'n02018795' 'n02025239' 'n02027492' 'n02028035' 'n02033041' 'n02037110'\n",
      " 'n02051845' 'n02056570' 'n02058221' 'n02066245' 'n02071294' 'n02074367'\n",
      " 'n02077923' 'n02085620' 'n02085782' 'n02085936' 'n02086079' 'n02086240'\n",
      " 'n02086646' 'n02086910' 'n02087046' 'n02087394' 'n02088094' 'n02088238'\n",
      " 'n02088364' 'n02088466' 'n02088632' 'n02089078' 'n02089867' 'n02089973'\n",
      " 'n02090379' 'n02090622' 'n02090721' 'n02091032' 'n02091134' 'n02091244'\n",
      " 'n02091467' 'n02091635' 'n02091831' 'n02092002' 'n02092339' 'n02093256'\n",
      " 'n02093428' 'n02093647' 'n02093754' 'n02093859' 'n02093991' 'n02094114'\n",
      " 'n02094258' 'n02094433' 'n02095314' 'n02095570' 'n02095889' 'n02096051'\n",
      " 'n02096177' 'n02096294' 'n02096437' 'n02096585' 'n02097047' 'n02097130'\n",
      " 'n02097209' 'n02097298' 'n02097474' 'n02097658' 'n02098105' 'n02098286'\n",
      " 'n02098413' 'n02099267' 'n02099429' 'n02099601' 'n02099712' 'n02099849'\n",
      " 'n02100236' 'n02100583' 'n02100735' 'n02100877' 'n02101006' 'n02101388'\n",
      " 'n02101556' 'n02102040' 'n02102177' 'n02102318' 'n02102480' 'n02102973'\n",
      " 'n02104029' 'n02104365' 'n02105056' 'n02105162' 'n02105251' 'n02105412'\n",
      " 'n02105505' 'n02105641' 'n02105855' 'n02106030' 'n02106166' 'n02106382'\n",
      " 'n02106550' 'n02106662' 'n02107142' 'n02107312' 'n02107574' 'n02107683'\n",
      " 'n02107908' 'n02108000' 'n02108089' 'n02108422' 'n02108551' 'n02108915'\n",
      " 'n02109047' 'n02109525' 'n02109961' 'n02110063' 'n02110185' 'n02110341'\n",
      " 'n02110627' 'n02110806' 'n02110958' 'n02111129' 'n02111277' 'n02111500'\n",
      " 'n02111889' 'n02112018' 'n02112137' 'n02112350' 'n02112706' 'n02113023'\n",
      " 'n02113186' 'n02113624' 'n02113712' 'n02113799' 'n02113978' 'n02114367'\n",
      " 'n02114548' 'n02114712' 'n02114855' 'n02115641' 'n02115913' 'n02116738'\n",
      " 'n02117135' 'n02119022' 'n02119789' 'n02120079' 'n02120505' 'n02123045'\n",
      " 'n02123159' 'n02123394' 'n02123597' 'n02124075' 'n02125311' 'n02127052'\n",
      " 'n02128385' 'n02128757' 'n02128925' 'n02129165' 'n02129604' 'n02130308'\n",
      " 'n02132136' 'n02133161' 'n02134084' 'n02134418' 'n02137549' 'n02138441'\n",
      " 'n02165105' 'n02165456' 'n02167151' 'n02168699' 'n02169497' 'n02172182'\n",
      " 'n02174001' 'n02177972' 'n02190166' 'n02206856' 'n02219486' 'n02226429'\n",
      " 'n02229544' 'n02231487' 'n02233338' 'n02236044' 'n02256656' 'n02259212'\n",
      " 'n02264363' 'n02268443' 'n02268853' 'n02276258' 'n02277742' 'n02279972'\n",
      " 'n02280649' 'n02281406' 'n02281787' 'n02317335' 'n02319095' 'n02321529'\n",
      " 'n02325366' 'n02326432' 'n02328150' 'n02342885' 'n02346627' 'n02356798'\n",
      " 'n02361337' 'n02363005' 'n02364673' 'n02389026' 'n02391049' 'n02395406'\n",
      " 'n02396427' 'n02397096' 'n02398521' 'n02403003' 'n02408429' 'n02410509'\n",
      " 'n02412080' 'n02415577' 'n02417914' 'n02422106' 'n02422699' 'n02423022'\n",
      " 'n02437312' 'n02437616' 'n02441942' 'n02442845' 'n02443114' 'n02443484'\n",
      " 'n02444819' 'n02445715' 'n02447366' 'n02454379' 'n02457408' 'n02480495'\n",
      " 'n02480855' 'n02481823' 'n02483362' 'n02483708' 'n02484975' 'n02486261'\n",
      " 'n02486410' 'n02487347' 'n02488291' 'n02488702' 'n02489166' 'n02490219'\n",
      " 'n02492035' 'n02492660' 'n02493509' 'n02493793' 'n02494079' 'n02497673'\n",
      " 'n02500267' 'n02504013' 'n02504458' 'n02509815' 'n02510455' 'n02514041'\n",
      " 'n02526121' 'n02536864' 'n02606052' 'n02607072' 'n02640242' 'n02641379'\n",
      " 'n02643566' 'n02655020' 'n02666196' 'n02667093' 'n02669723' 'n02672831'\n",
      " 'n02676566' 'n02687172' 'n02690373' 'n02692877' 'n02699494' 'n02701002'\n",
      " 'n02704792' 'n02708093' 'n02727426' 'n02730930' 'n02747177' 'n02749479'\n",
      " 'n02769748' 'n02776631' 'n02777292' 'n02782093' 'n02783161' 'n02786058'\n",
      " 'n02787622' 'n02788148' 'n02790996' 'n02791124' 'n02791270' 'n02793495'\n",
      " 'n02794156' 'n02795169' 'n02797295' 'n02799071' 'n02802426' 'n02804414'\n",
      " 'n02804610' 'n02807133' 'n02808304' 'n02808440' 'n02814533' 'n02814860'\n",
      " 'n02815834' 'n02817516' 'n02823428' 'n02823750' 'n02825657' 'n02834397'\n",
      " 'n02835271' 'n02837789' 'n02840245' 'n02841315' 'n02843684' 'n02859443'\n",
      " 'n02860847' 'n02865351' 'n02869837' 'n02870880' 'n02871525' 'n02877765'\n",
      " 'n02879718' 'n02883205' 'n02892201' 'n02892767' 'n02894605' 'n02895154'\n",
      " 'n02906734' 'n02909870' 'n02910353' 'n02916936' 'n02917067' 'n02927161'\n",
      " 'n02930766' 'n02939185' 'n02948072' 'n02950826' 'n02951358' 'n02951585'\n",
      " 'n02963159' 'n02965783' 'n02966193' 'n02966687' 'n02971356' 'n02974003'\n",
      " 'n02977058' 'n02978881' 'n02979186' 'n02980441' 'n02981792' 'n02988304'\n",
      " 'n02992211' 'n02992529' 'n02999410' 'n03000134' 'n03000247' 'n03000684'\n",
      " 'n03014705' 'n03016953' 'n03017168' 'n03018349' 'n03026506' 'n03028079'\n",
      " 'n03032252' 'n03041632' 'n03042490' 'n03045698' 'n03047690' 'n03062245'\n",
      " 'n03063599' 'n03063689' 'n03065424' 'n03075370' 'n03085013' 'n03089624'\n",
      " 'n03095699' 'n03100240' 'n03109150' 'n03110669' 'n03124043' 'n03124170'\n",
      " 'n03125729' 'n03126707' 'n03127747' 'n03127925' 'n03131574' 'n03133878'\n",
      " 'n03134739' 'n03141823' 'n03146219' 'n03160309' 'n03179701' 'n03180011'\n",
      " 'n03187595' 'n03188531' 'n03196217' 'n03197337' 'n03201208' 'n03207743'\n",
      " 'n03207941' 'n03208938' 'n03216828' 'n03218198' 'n03220513' 'n03223299'\n",
      " 'n03240683' 'n03249569' 'n03250847' 'n03255030' 'n03259280' 'n03271574'\n",
      " 'n03272010' 'n03272562' 'n03290653' 'n03291819' 'n03297495' 'n03314780'\n",
      " 'n03325584' 'n03337140' 'n03344393' 'n03345487' 'n03347037' 'n03355925'\n",
      " 'n03372029' 'n03376595' 'n03379051' 'n03384352' 'n03388043' 'n03388183'\n",
      " 'n03388549' 'n03393912' 'n03394916' 'n03400231' 'n03404251' 'n03417042'\n",
      " 'n03424325' 'n03425413' 'n03443371' 'n03444034' 'n03445777' 'n03445924'\n",
      " 'n03447447' 'n03447721' 'n03450230' 'n03452741' 'n03457902' 'n03459775'\n",
      " 'n03461385' 'n03467068' 'n03476684' 'n03476991' 'n03478589' 'n03481172'\n",
      " 'n03482405' 'n03483316' 'n03485407' 'n03485794' 'n03492542' 'n03494278'\n",
      " 'n03495258' 'n03496892' 'n03498962' 'n03527444' 'n03529860' 'n03530642'\n",
      " 'n03532672' 'n03534580' 'n03535780' 'n03538406' 'n03544143' 'n03584254'\n",
      " 'n03584829' 'n03590841' 'n03594734' 'n03594945' 'n03595614' 'n03598930'\n",
      " 'n03599486' 'n03602883' 'n03617480' 'n03623198' 'n03627232' 'n03630383'\n",
      " 'n03633091' 'n03637318' 'n03642806' 'n03649909' 'n03657121' 'n03658185'\n",
      " 'n03661043' 'n03662601' 'n03666591' 'n03670208' 'n03673027' 'n03676483'\n",
      " 'n03680355' 'n03690938' 'n03691459' 'n03692522' 'n03697007' 'n03706229'\n",
      " 'n03709823' 'n03710193' 'n03710637' 'n03710721' 'n03717622' 'n03720891'\n",
      " 'n03721384' 'n03724870' 'n03729826' 'n03733131' 'n03733281' 'n03733805'\n",
      " 'n03742115' 'n03743016' 'n03759954' 'n03761084' 'n03763968' 'n03764736'\n",
      " 'n03769881' 'n03770439' 'n03770679' 'n03773504' 'n03775071' 'n03775546'\n",
      " 'n03776460' 'n03777568' 'n03777754' 'n03781244' 'n03782006' 'n03785016'\n",
      " 'n03786901' 'n03787032' 'n03788195' 'n03788365' 'n03791053' 'n03792782'\n",
      " 'n03792972' 'n03793489' 'n03794056' 'n03796401' 'n03803284' 'n03804744'\n",
      " 'n03814639' 'n03814906' 'n03825788' 'n03832673' 'n03837869' 'n03838899'\n",
      " 'n03840681' 'n03841143' 'n03843555' 'n03854065' 'n03857828' 'n03866082'\n",
      " 'n03868242' 'n03868863' 'n03871628' 'n03873416' 'n03874293' 'n03874599'\n",
      " 'n03876231' 'n03877472' 'n03877845' 'n03884397' 'n03887697' 'n03888257'\n",
      " 'n03888605' 'n03891251' 'n03891332' 'n03895866' 'n03899768' 'n03902125'\n",
      " 'n03903868' 'n03908618' 'n03908714' 'n03916031' 'n03920288' 'n03924679'\n",
      " 'n03929660' 'n03929855' 'n03930313' 'n03930630' 'n03933933' 'n03935335'\n",
      " 'n03937543' 'n03938244' 'n03942813' 'n03944341' 'n03947888' 'n03950228'\n",
      " 'n03954731' 'n03956157' 'n03958227' 'n03961711' 'n03967562' 'n03970156'\n",
      " 'n03976467' 'n03976657' 'n03977966' 'n03980874' 'n03982430' 'n03983396'\n",
      " 'n03991062' 'n03992509' 'n03995372' 'n03998194' 'n04004767' 'n04005630'\n",
      " 'n04008634' 'n04009552' 'n04019541' 'n04023962' 'n04026417' 'n04033901'\n",
      " 'n04033995' 'n04037443' 'n04039381' 'n04040759' 'n04041544' 'n04044716'\n",
      " 'n04049303' 'n04065272' 'n04067472' 'n04069434' 'n04070727' 'n04074963'\n",
      " 'n04081281' 'n04086273' 'n04090263' 'n04099969' 'n04111531' 'n04116512'\n",
      " 'n04118538' 'n04118776' 'n04120489' 'n04125021' 'n04127249' 'n04131690'\n",
      " 'n04133789' 'n04136333' 'n04141076' 'n04141327' 'n04141975' 'n04146614'\n",
      " 'n04147183' 'n04149813' 'n04152593' 'n04153751' 'n04154565' 'n04162706'\n",
      " 'n04179913' 'n04192698' 'n04200800' 'n04201297' 'n04204238' 'n04204347'\n",
      " 'n04208210' 'n04209133' 'n04209239' 'n04228054' 'n04229816' 'n04235860'\n",
      " 'n04238763' 'n04239074' 'n04243546' 'n04251144' 'n04252077' 'n04252225'\n",
      " 'n04254120' 'n04254680' 'n04254777' 'n04258138' 'n04259630' 'n04263257'\n",
      " 'n04264628' 'n04265275' 'n04266014' 'n04270147' 'n04273569' 'n04275548'\n",
      " 'n04277352' 'n04285008' 'n04286575' 'n04296562' 'n04310018' 'n04311004'\n",
      " 'n04311174' 'n04317175' 'n04325704' 'n04326547' 'n04328186' 'n04330267'\n",
      " 'n04332243' 'n04335435' 'n04336792' 'n04344873' 'n04346328' 'n04347754'\n",
      " 'n04350905' 'n04355338' 'n04355933' 'n04356056' 'n04357314' 'n04366367'\n",
      " 'n04367480' 'n04370456' 'n04371430' 'n04371774' 'n04372370' 'n04376876'\n",
      " 'n04380533' 'n04389033' 'n04392985' 'n04398044' 'n04399382' 'n04404412'\n",
      " 'n04409515' 'n04417672' 'n04418357' 'n04423845' 'n04428191' 'n04429376'\n",
      " 'n04435653' 'n04442312' 'n04443257' 'n04447861' 'n04456115' 'n04458633'\n",
      " 'n04461696' 'n04462240' 'n04465501' 'n04467665' 'n04476259' 'n04479046'\n",
      " 'n04482393' 'n04483307' 'n04485082' 'n04486054' 'n04487081' 'n04487394'\n",
      " 'n04493381' 'n04501370' 'n04505470' 'n04507155' 'n04509417' 'n04515003'\n",
      " 'n04517823' 'n04522168' 'n04523525' 'n04525038' 'n04525305' 'n04532106'\n",
      " 'n04532670' 'n04536866' 'n04540053' 'n04542943' 'n04548280' 'n04548362'\n",
      " 'n04550184' 'n04552348' 'n04553703' 'n04554684' 'n04557648' 'n04560804'\n",
      " 'n04562935' 'n04579145' 'n04579432' 'n04584207' 'n04589890' 'n04590129'\n",
      " 'n04591157' 'n04591713' 'n04592741' 'n04596742' 'n04597913' 'n04599235'\n",
      " 'n04604644' 'n04606251' 'n04612504' 'n04613696' 'n06359193' 'n06596364'\n",
      " 'n06785654' 'n06794110' 'n06874185' 'n07248320' 'n07565083' 'n07579787'\n",
      " 'n07583066' 'n07584110' 'n07590611' 'n07613480' 'n07614500' 'n07615774'\n",
      " 'n07684084' 'n07693725' 'n07695742' 'n07697313' 'n07697537' 'n07711569'\n",
      " 'n07714571' 'n07714990' 'n07715103' 'n07716358' 'n07716906' 'n07717410'\n",
      " 'n07717556' 'n07718472' 'n07718747' 'n07720875' 'n07730033' 'n07734744'\n",
      " 'n07742313' 'n07745940' 'n07747607' 'n07749582' 'n07753113' 'n07753275'\n",
      " 'n07753592' 'n07754684' 'n07760859' 'n07768694' 'n07802026' 'n07831146'\n",
      " 'n07836838' 'n07860988' 'n07871810' 'n07873807' 'n07875152' 'n07880968'\n",
      " 'n07892512' 'n07920052' 'n07930864' 'n07932039' 'n09193705' 'n09229709'\n",
      " 'n09246464' 'n09256479' 'n09288635' 'n09332890' 'n09399592' 'n09421951'\n",
      " 'n09428293' 'n09468604' 'n09472597' 'n09835506' 'n10148035' 'n10565667'\n",
      " 'n11879895' 'n11939491' 'n12057211' 'n12144580' 'n12267677' 'n12620546'\n",
      " 'n12768682' 'n12985857' 'n12998815' 'n13037406' 'n13040303' 'n13044778'\n",
      " 'n13052670' 'n13054560' 'n13133613' 'n15075141']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1200/1200 [02:03<00:00,  9.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_workers = 1\n",
    "worker_number = 0\n",
    "\n",
    "all_classes = [\"n01440764\", \"n01443537\", \"n01484850\", \"n01491361\", \"n01494475\", \"n01496331\", \"n01498041\", \"n01514668\", \"n01514859\", \"n01518878\", \"n01530575\", \"n01531178\", \"n01532829\", \"n01534433\", \"n01537544\", \"n01558993\", \"n01560419\", \"n01580077\", \"n01582220\", \"n01592084\", \"n01601694\", \"n01608432\", \"n01614925\", \"n01616318\", \"n01622779\", \"n01629819\", \"n01630670\", \"n01631663\", \"n01632458\", \"n01632777\", \"n01641577\", \"n01644373\", \"n01644900\", \"n01664065\", \"n01665541\", \"n01667114\", \"n01667778\", \"n01669191\", \"n01675722\", \"n01677366\", \"n01682714\", \"n01685808\", \"n01687978\", \"n01688243\", \"n01689811\", \"n01692333\", \"n01693334\", \"n01694178\", \"n01695060\", \"n01697457\", \"n01698640\", \"n01704323\", \"n01728572\", \"n01728920\", \"n01729322\", \"n01729977\", \"n01734418\", \"n01735189\", \"n01737021\", \"n01739381\", \"n01740131\", \"n01742172\", \"n01744401\", \"n01748264\", \"n01749939\", \"n01751748\", \"n01753488\", \"n01755581\", \"n01756291\", \"n01768244\", \"n01770081\", \"n01770393\", \"n01773157\", \"n01773549\", \"n01773797\", \"n01774384\", \"n01774750\", \"n01775062\", \"n01776313\", \"n01784675\", \"n01795545\", \"n01796340\", \"n01797886\", \"n01798484\", \"n01806143\", \"n01806567\", \"n01807496\", \"n01817953\", \"n01818515\", \"n01819313\", \"n01820546\", \"n01824575\", \"n01828970\", \"n01829413\", \"n01833805\", \"n01843065\", \"n01843383\", \"n01847000\", \"n01855032\", \"n01855672\", \"n01860187\", \"n01871265\", \"n01872401\", \"n01873310\", \"n01877812\", \"n01882714\", \"n01883070\", \"n01910747\", \"n01914609\", \"n01917289\", \"n01924916\", \"n01930112\", \"n01943899\", \"n01944390\", \"n01945685\", \"n01950731\", \"n01955084\", \"n01968897\", \"n01978287\", \"n01978455\", \"n01980166\", \"n01981276\", \"n01983481\", \"n01984695\", \"n01985128\", \"n01986214\", \"n01990800\", \"n02002556\", \"n02002724\", \"n02006656\", \"n02007558\", \"n02009229\", \"n02009912\", \"n02011460\", \"n02012849\", \"n02013706\", \"n02017213\", \"n02018207\", \"n02018795\", \"n02025239\", \"n02027492\", \"n02028035\", \"n02033041\", \"n02037110\", \"n02051845\", \"n02056570\", \"n02058221\", \"n02066245\", \"n02071294\", \"n02074367\", \"n02077923\", \"n02085620\", \"n02085782\", \"n02085936\", \"n02086079\", \"n02086240\", \"n02086646\", \"n02086910\", \"n02087046\", \"n02087394\", \"n02088094\", \"n02088238\", \"n02088364\", \"n02088466\", \"n02088632\", \"n02089078\", \"n02089867\", \"n02089973\", \"n02090379\", \"n02090622\", \"n02090721\", \"n02091032\", \"n02091134\", \"n02091244\", \"n02091467\", \"n02091635\", \"n02091831\", \"n02092002\", \"n02092339\", \"n02093256\", \"n02093428\", \"n02093647\", \"n02093754\", \"n02093859\", \"n02093991\", \"n02094114\", \"n02094258\", \"n02094433\", \"n02095314\", \"n02095570\", \"n02095889\", \"n02096051\", \"n02096177\", \"n02096294\", \"n02096437\", \"n02096585\", \"n02097047\", \"n02097130\", \"n02097209\", \"n02097298\", \"n02097474\", \"n02097658\", \"n02098105\", \"n02098286\", \"n02098413\", \"n02099267\", \"n02099429\", \"n02099601\", \"n02099712\", \"n02099849\", \"n02100236\", \"n02100583\", \"n02100735\", \"n02100877\", \"n02101006\", \"n02101388\", \"n02101556\", \"n02102040\", \"n02102177\", \"n02102318\", \"n02102480\", \"n02102973\", \"n02104029\", \"n02104365\", \"n02105056\", \"n02105162\", \"n02105251\", \"n02105412\", \"n02105505\", \"n02105641\", \"n02105855\", \"n02106030\", \"n02106166\", \"n02106382\", \"n02106550\", \"n02106662\", \"n02107142\", \"n02107312\", \"n02107574\", \"n02107683\", \"n02107908\", \"n02108000\", \"n02108089\", \"n02108422\", \"n02108551\", \"n02108915\", \"n02109047\", \"n02109525\", \"n02109961\", \"n02110063\", \"n02110185\", \"n02110341\", \"n02110627\", \"n02110806\", \"n02110958\", \"n02111129\", \"n02111277\", \"n02111500\", \"n02111889\", \"n02112018\", \"n02112137\", \"n02112350\", \"n02112706\", \"n02113023\", \"n02113186\", \"n02113624\", \"n02113712\", \"n02113799\", \"n02113978\", \"n02114367\", \"n02114548\", \"n02114712\", \"n02114855\", \"n02115641\", \"n02115913\", \"n02116738\", \"n02117135\", \"n02119022\", \"n02119789\", \"n02120079\", \"n02120505\", \"n02123045\", \"n02123159\", \"n02123394\", \"n02123597\", \"n02124075\", \"n02125311\", \"n02127052\", \"n02128385\", \"n02128757\", \"n02128925\", \"n02129165\", \"n02129604\", \"n02130308\", \"n02132136\", \"n02133161\", \"n02134084\", \"n02134418\", \"n02137549\", \"n02138441\", \"n02165105\", \"n02165456\", \"n02167151\", \"n02168699\", \"n02169497\", \"n02172182\", \"n02174001\", \"n02177972\", \"n02190166\", \"n02206856\", \"n02219486\", \"n02226429\", \"n02229544\", \"n02231487\", \"n02233338\", \"n02236044\", \"n02256656\", \"n02259212\", \"n02264363\", \"n02268443\", \"n02268853\", \"n02276258\", \"n02277742\", \"n02279972\", \"n02280649\", \"n02281406\", \"n02281787\", \"n02317335\", \"n02319095\", \"n02321529\", \"n02325366\", \"n02326432\", \"n02328150\", \"n02342885\", \"n02346627\", \"n02356798\", \"n02361337\", \"n02363005\", \"n02364673\", \"n02389026\", \"n02391049\", \"n02395406\", \"n02396427\", \"n02397096\", \"n02398521\", \"n02403003\", \"n02408429\", \"n02410509\", \"n02412080\", \"n02415577\", \"n02417914\", \"n02422106\", \"n02422699\", \"n02423022\", \"n02437312\", \"n02437616\", \"n02441942\", \"n02442845\", \"n02443114\", \"n02443484\", \"n02444819\", \"n02445715\", \"n02447366\", \"n02454379\", \"n02457408\", \"n02480495\", \"n02480855\", \"n02481823\", \"n02483362\", \"n02483708\", \"n02484975\", \"n02486261\", \"n02486410\", \"n02487347\", \"n02488291\", \"n02488702\", \"n02489166\", \"n02490219\", \"n02492035\", \"n02492660\", \"n02493509\", \"n02493793\", \"n02494079\", \"n02497673\", \"n02500267\", \"n02504013\", \"n02504458\", \"n02509815\", \"n02510455\", \"n02514041\", \"n02526121\", \"n02536864\", \"n02606052\", \"n02607072\", \"n02640242\", \"n02641379\", \"n02643566\", \"n02655020\", \"n02666196\", \"n02667093\", \"n02669723\", \"n02672831\", \"n02676566\", \"n02687172\", \"n02690373\", \"n02692877\", \"n02699494\", \"n02701002\", \"n02704792\", \"n02708093\", \"n02727426\", \"n02730930\", \"n02747177\", \"n02749479\", \"n02769748\", \"n02776631\", \"n02777292\", \"n02782093\", \"n02783161\", \"n02786058\", \"n02787622\", \"n02788148\", \"n02790996\", \"n02791124\", \"n02791270\", \"n02793495\", \"n02794156\", \"n02795169\", \"n02797295\", \"n02799071\", \"n02802426\", \"n02804414\", \"n02804610\", \"n02807133\", \"n02808304\", \"n02808440\", \"n02814533\", \"n02814860\", \"n02815834\", \"n02817516\", \"n02823428\", \"n02823750\", \"n02825657\", \"n02834397\", \"n02835271\", \"n02837789\", \"n02840245\", \"n02841315\", \"n02843684\", \"n02859443\", \"n02860847\", \"n02865351\", \"n02869837\", \"n02870880\", \"n02871525\", \"n02877765\", \"n02879718\", \"n02883205\", \"n02892201\", \"n02892767\", \"n02894605\", \"n02895154\", \"n02906734\", \"n02909870\", \"n02910353\", \"n02916936\", \"n02917067\", \"n02927161\", \"n02930766\", \"n02939185\", \"n02948072\", \"n02950826\", \"n02951358\", \"n02951585\", \"n02963159\", \"n02965783\", \"n02966193\", \"n02966687\", \"n02971356\", \"n02974003\", \"n02977058\", \"n02978881\", \"n02979186\", \"n02980441\", \"n02981792\", \"n02988304\", \"n02992211\", \"n02992529\", \"n02999410\", \"n03000134\", \"n03000247\", \"n03000684\", \"n03014705\", \"n03016953\", \"n03017168\", \"n03018349\", \"n03026506\", \"n03028079\", \"n03032252\", \"n03041632\", \"n03042490\", \"n03045698\", \"n03047690\", \"n03062245\", \"n03063599\", \"n03063689\", \"n03065424\", \"n03075370\", \"n03085013\", \"n03089624\", \"n03095699\", \"n03100240\", \"n03109150\", \"n03110669\", \"n03124043\", \"n03124170\", \"n03125729\", \"n03126707\", \"n03127747\", \"n03127925\", \"n03131574\", \"n03133878\", \"n03134739\", \"n03141823\", \"n03146219\", \"n03160309\", \"n03179701\", \"n03180011\", \"n03187595\", \"n03188531\", \"n03196217\", \"n03197337\", \"n03201208\", \"n03207743\", \"n03207941\", \"n03208938\", \"n03216828\", \"n03218198\", \"n03220513\", \"n03223299\", \"n03240683\", \"n03249569\", \"n03250847\", \"n03255030\", \"n03259280\", \"n03271574\", \"n03272010\", \"n03272562\", \"n03290653\", \"n03291819\", \"n03297495\", \"n03314780\", \"n03325584\", \"n03337140\", \"n03344393\", \"n03345487\", \"n03347037\", \"n03355925\", \"n03372029\", \"n03376595\", \"n03379051\", \"n03384352\", \"n03388043\", \"n03388183\", \"n03388549\", \"n03393912\", \"n03394916\", \"n03400231\", \"n03404251\", \"n03417042\", \"n03424325\", \"n03425413\", \"n03443371\", \"n03444034\", \"n03445777\", \"n03445924\", \"n03447447\", \"n03447721\", \"n03450230\", \"n03452741\", \"n03457902\", \"n03459775\", \"n03461385\", \"n03467068\", \"n03476684\", \"n03476991\", \"n03478589\", \"n03481172\", \"n03482405\", \"n03483316\", \"n03485407\", \"n03485794\", \"n03492542\", \"n03494278\", \"n03495258\", \"n03496892\", \"n03498962\", \"n03527444\", \"n03529860\", \"n03530642\", \"n03532672\", \"n03534580\", \"n03535780\", \"n03538406\", \"n03544143\", \"n03584254\", \"n03584829\", \"n03590841\", \"n03594734\", \"n03594945\", \"n03595614\", \"n03598930\", \"n03599486\", \"n03602883\", \"n03617480\", \"n03623198\", \"n03627232\", \"n03630383\", \"n03633091\", \"n03637318\", \"n03642806\", \"n03649909\", \"n03657121\", \"n03658185\", \"n03661043\", \"n03662601\", \"n03666591\", \"n03670208\", \"n03673027\", \"n03676483\", \"n03680355\", \"n03690938\", \"n03691459\", \"n03692522\", \"n03697007\", \"n03706229\", \"n03709823\", \"n03710193\", \"n03710637\", \"n03710721\", \"n03717622\", \"n03720891\", \"n03721384\", \"n03724870\", \"n03729826\", \"n03733131\", \"n03733281\", \"n03733805\", \"n03742115\", \"n03743016\", \"n03759954\", \"n03761084\", \"n03763968\", \"n03764736\", \"n03769881\", \"n03770439\", \"n03770679\", \"n03773504\", \"n03775071\", \"n03775546\", \"n03776460\", \"n03777568\", \"n03777754\", \"n03781244\", \"n03782006\", \"n03785016\", \"n03786901\", \"n03787032\", \"n03788195\", \"n03788365\", \"n03791053\", \"n03792782\", \"n03792972\", \"n03793489\", \"n03794056\", \"n03796401\", \"n03803284\", \"n03804744\", \"n03814639\", \"n03814906\", \"n03825788\", \"n03832673\", \"n03837869\", \"n03838899\", \"n03840681\", \"n03841143\", \"n03843555\", \"n03854065\", \"n03857828\", \"n03866082\", \"n03868242\", \"n03868863\", \"n03871628\", \"n03873416\", \"n03874293\", \"n03874599\", \"n03876231\", \"n03877472\", \"n03877845\", \"n03884397\", \"n03887697\", \"n03888257\", \"n03888605\", \"n03891251\", \"n03891332\", \"n03895866\", \"n03899768\", \"n03902125\", \"n03903868\", \"n03908618\", \"n03908714\", \"n03916031\", \"n03920288\", \"n03924679\", \"n03929660\", \"n03929855\", \"n03930313\", \"n03930630\", \"n03933933\", \"n03935335\", \"n03937543\", \"n03938244\", \"n03942813\", \"n03944341\", \"n03947888\", \"n03950228\", \"n03954731\", \"n03956157\", \"n03958227\", \"n03961711\", \"n03967562\", \"n03970156\", \"n03976467\", \"n03976657\", \"n03977966\", \"n03980874\", \"n03982430\", \"n03983396\", \"n03991062\", \"n03992509\", \"n03995372\", \"n03998194\", \"n04004767\", \"n04005630\", \"n04008634\", \"n04009552\", \"n04019541\", \"n04023962\", \"n04026417\", \"n04033901\", \"n04033995\", \"n04037443\", \"n04039381\", \"n04040759\", \"n04041544\", \"n04044716\", \"n04049303\", \"n04065272\", \"n04067472\", \"n04069434\", \"n04070727\", \"n04074963\", \"n04081281\", \"n04086273\", \"n04090263\", \"n04099969\", \"n04111531\", \"n04116512\", \"n04118538\", \"n04118776\", \"n04120489\", \"n04125021\", \"n04127249\", \"n04131690\", \"n04133789\", \"n04136333\", \"n04141076\", \"n04141327\", \"n04141975\", \"n04146614\", \"n04147183\", \"n04149813\", \"n04152593\", \"n04153751\", \"n04154565\", \"n04162706\", \"n04179913\", \"n04192698\", \"n04200800\", \"n04201297\", \"n04204238\", \"n04204347\", \"n04208210\", \"n04209133\", \"n04209239\", \"n04228054\", \"n04229816\", \"n04235860\", \"n04238763\", \"n04239074\", \"n04243546\", \"n04251144\", \"n04252077\", \"n04252225\", \"n04254120\", \"n04254680\", \"n04254777\", \"n04258138\", \"n04259630\", \"n04263257\", \"n04264628\", \"n04265275\", \"n04266014\", \"n04270147\", \"n04273569\", \"n04275548\", \"n04277352\", \"n04285008\", \"n04286575\", \"n04296562\", \"n04310018\", \"n04311004\", \"n04311174\", \"n04317175\", \"n04325704\", \"n04326547\", \"n04328186\", \"n04330267\", \"n04332243\", \"n04335435\", \"n04336792\", \"n04344873\", \"n04346328\", \"n04347754\", \"n04350905\", \"n04355338\", \"n04355933\", \"n04356056\", \"n04357314\", \"n04366367\", \"n04367480\", \"n04370456\", \"n04371430\", \"n04371774\", \"n04372370\", \"n04376876\", \"n04380533\", \"n04389033\", \"n04392985\", \"n04398044\", \"n04399382\", \"n04404412\", \"n04409515\", \"n04417672\", \"n04418357\", \"n04423845\", \"n04428191\", \"n04429376\", \"n04435653\", \"n04442312\", \"n04443257\", \"n04447861\", \"n04456115\", \"n04458633\", \"n04461696\", \"n04462240\", \"n04465501\", \"n04467665\", \"n04476259\", \"n04479046\", \"n04482393\", \"n04483307\", \"n04485082\", \"n04486054\", \"n04487081\", \"n04487394\", \"n04493381\", \"n04501370\", \"n04505470\", \"n04507155\", \"n04509417\", \"n04515003\", \"n04517823\", \"n04522168\", \"n04523525\", \"n04525038\", \"n04525305\", \"n04532106\", \"n04532670\", \"n04536866\", \"n04540053\", \"n04542943\", \"n04548280\", \"n04548362\", \"n04550184\", \"n04552348\", \"n04553703\", \"n04554684\", \"n04557648\", \"n04560804\", \"n04562935\", \"n04579145\", \"n04579432\", \"n04584207\", \"n04589890\", \"n04590129\", \"n04591157\", \"n04591713\", \"n04592741\", \"n04596742\", \"n04597913\", \"n04599235\", \"n04604644\", \"n04606251\", \"n04612504\", \"n04613696\", \"n06359193\", \"n06596364\", \"n06785654\", \"n06794110\", \"n06874185\", \"n07248320\", \"n07565083\", \"n07579787\", \"n07583066\", \"n07584110\", \"n07590611\", \"n07613480\", \"n07614500\", \"n07615774\", \"n07684084\", \"n07693725\", \"n07695742\", \"n07697313\", \"n07697537\", \"n07711569\", \"n07714571\", \"n07714990\", \"n07715103\", \"n07716358\", \"n07716906\", \"n07717410\", \"n07717556\", \"n07718472\", \"n07718747\", \"n07720875\", \"n07730033\", \"n07734744\", \"n07742313\", \"n07745940\", \"n07747607\", \"n07749582\", \"n07753113\", \"n07753275\", \"n07753592\", \"n07754684\", \"n07760859\", \"n07768694\", \"n07802026\", \"n07831146\", \"n07836838\", \"n07860988\", \"n07871810\", \"n07873807\", \"n07875152\", \"n07880968\", \"n07892512\", \"n07920052\", \"n07930864\", \"n07932039\", \"n09193705\", \"n09229709\", \"n09246464\", \"n09256479\", \"n09288635\", \"n09332890\", \"n09399592\", \"n09421951\", \"n09428293\", \"n09468604\", \"n09472597\", \"n09835506\", \"n10148035\", \"n10565667\", \"n11879895\", \"n11939491\", \"n12057211\", \"n12144580\", \"n12267677\", \"n12620546\", \"n12768682\", \"n12985857\", \"n12998815\", \"n13037406\", \"n13040303\", \"n13044778\", \"n13052670\", \"n13054560\", \"n13133613\", \"n15075141\"]\n",
    "all_classes.sort()\n",
    "assert len(all_classes) == 1000\n",
    "\n",
    "# Subset for this worker\n",
    "classes_chosen = np.array_split(all_classes,total_workers)[worker_number]\n",
    "\n",
    "class ImageNetSubsetDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class to take a specified subset of some larger dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, root, *args, **kwargs):\n",
    "        \n",
    "        print(\"Using {0} classes {1}\".format(len(classes_chosen), classes_chosen))\n",
    "\n",
    "        self.new_root = tempfile.mkdtemp()\n",
    "        for _class in classes_chosen:\n",
    "            orig_dir = os.path.join(root, _class)\n",
    "            #assert os.path.isdir(orig_dir)\n",
    "            \n",
    "            os.symlink(orig_dir, os.path.join(self.new_root, _class))\n",
    "        \n",
    "        super().__init__(self.new_root, *args, **kwargs)\n",
    "\n",
    "        return self.new_root\n",
    "    \n",
    "    def __del__(self):\n",
    "        # Clean up\n",
    "        shutil.rmtree(self.new_root)\n",
    "\n",
    "\n",
    "test_transform = trn.Compose([trn.Resize(512), trn.ToTensor()])\n",
    "\n",
    "def get_weights():\n",
    "    weight_keys = ['e_conv_1.1.weight', 'e_conv_1.1.bias', 'e_conv_2.1.weight', 'e_conv_2.1.bias', 'e_block_1.1.weight', 'e_block_1.1.bias', 'e_block_1.4.weight', 'e_block_1.4.bias', 'e_block_2.1.weight', 'e_block_2.1.bias', 'e_block_2.4.weight', 'e_block_2.4.bias', 'e_block_3.1.weight', 'e_block_3.1.bias', 'e_block_3.4.weight', 'e_block_3.4.bias', 'e_conv_3.0.weight', 'e_conv_3.0.bias', 'd_up_conv_1.0.weight', 'd_up_conv_1.0.bias', 'd_up_conv_1.3.weight', 'd_up_conv_1.3.bias', 'd_block_1.1.weight', 'd_block_1.1.bias', 'd_block_1.4.weight', 'd_block_1.4.bias', 'd_block_2.1.weight', 'd_block_2.1.bias', 'd_block_2.4.weight', 'd_block_2.4.bias', 'd_block_3.1.weight', 'd_block_3.1.bias', 'd_block_3.4.weight', 'd_block_3.4.bias', 'd_up_conv_2.0.weight', 'd_up_conv_2.0.bias', 'd_up_conv_2.3.weight', 'd_up_conv_2.3.bias', 'd_up_conv_3.0.weight', 'd_up_conv_3.0.bias', 'd_up_conv_3.3.weight', 'd_up_conv_3.3.bias']\n",
    "    key_mapping = dict([(str(int(i / 2)) + \".weight\", key) if i % 2 == 0 else (str(int(i / 2)) + \".bias\", key) for i, key in enumerate(weight_keys)])\n",
    "    NUM_LAYERS = int(len(key_mapping.values()) / 2) # 21\n",
    "    NUM_DISTORTIONS = 8\n",
    "    MODEL_PATH = \"CAE_Weights/model_final.state\"\n",
    "    OPTION_LAYER_MAPPING = {0: range(11, NUM_LAYERS - 5), 1: range(8, NUM_LAYERS - 7), 2: range(8, NUM_LAYERS - 7), 3: range(10, NUM_LAYERS - 7), 4: range(8, NUM_LAYERS - 7), 5: range(8, NUM_LAYERS - 7), 6: range(8, NUM_LAYERS - 7), 7: range(8, NUM_LAYERS - 7), 8: range(8, NUM_LAYERS - 7)}\n",
    "\n",
    "    def get_name(i, tpe):\n",
    "        return key_mapping[str(i) + \".\" + tpe]\n",
    "\n",
    "    weights = torch.load(MODEL_PATH)\n",
    "    for option in random.sample(range(NUM_DISTORTIONS), 1):\n",
    "        i = np.random.choice(OPTION_LAYER_MAPPING[option])\n",
    "        j = np.random.choice(OPTION_LAYER_MAPPING[option])\n",
    "        weight_i = get_name(i, \"weight\")\n",
    "        bias_i = get_name(i, \"bias\")\n",
    "        weight_j = get_name(j, \"weight\")\n",
    "        bias_j = get_name(j, \"weight\")\n",
    "        if option == 0:\n",
    "            weights[weight_i] = torch.flip(weights[weight_i], (0,))\n",
    "            weights[bias_i] = torch.flip(weights[bias_i], (0,))\n",
    "            weights[weight_j] = torch.flip(weights[weight_j], (0,))\n",
    "            weights[bias_j] = torch.flip(weights[bias_j], (0,))\n",
    "        elif option == 1:\n",
    "            for k in [np.random.choice(weights[weight_i].size()[0]) for _ in range(12)]:\n",
    "                weights[weight_i][k] = -weights[weight_i][k]\n",
    "                weights[bias_i][k] = -weights[bias_i][k]\n",
    "        elif option == 2:\n",
    "            for k in [np.random.choice(weights[weight_i].size()[0]) for _ in range(25)]:\n",
    "                weights[weight_i][k] = 0 * weights[weight_i][k]\n",
    "                weights[bias_i][k] = 0 * weights[bias_i][k]\n",
    "        elif option == 3:\n",
    "            for k in [np.random.choice(weights[weight_i].size()[0]) for _ in range(25)]:\n",
    "                weights[weight_i][k] = -gelu(weights[weight_i][k])\n",
    "                weights[bias_i][k] = -gelu(weights[bias_i][k])\n",
    "        elif option == 4:\n",
    "            weights[weight_i] = weights[weight_i] *\\\n",
    "            (1 + 2 * np.float32(np.random.uniform()) * (4*torch.rand_like(weights[weight_i]-1)))\n",
    "            weights[weight_j] = weights[weight_j] *\\\n",
    "            (1 + 2 * np.float32(np.random.uniform()) * (4*torch.rand_like(weights[weight_j]-1)))\n",
    "        elif option == 5: ##### begin saurav #####\n",
    "            if random.random() < 0.5:\n",
    "                mask = torch.round(torch.rand_like(weights[weight_i]))\n",
    "            else:\n",
    "                mask = torch.round(torch.rand_like(weights[weight_i])) * 2 - 1\n",
    "            weights[weight_i] *= mask\n",
    "        elif option == 6:\n",
    "            _k = random.randint(1, 3)\n",
    "            weights[weight_i] = torch.rot90(weights[weight_i], k=_k, dims=(2,3))\n",
    "        elif option == 7:\n",
    "            out_filters = weights[weight_i].shape[0]\n",
    "            to_zero = list(set([random.choice(list(range(out_filters))) for _ in range(out_filters // 5)]))\n",
    "            weights[weight_i][to_zero] = weights[weight_i][to_zero] * -1.0\n",
    "        elif option == 8:\n",
    "            # Only keep the max filter value in the conv \n",
    "            c1, c2, width = weights[weight_i].shape[0], weights[weight_i].shape[1], weights[weight_i].shape[2]\n",
    "            assert weights[weight_i].shape[2] == weights[weight_i].shape[3]\n",
    "\n",
    "            w = torch.reshape(weights[weight_i], shape=(c1, c2, width ** 2))\n",
    "            res = torch.topk(w, k=1)\n",
    "\n",
    "            w_new = torch.zeros_like(w).scatter(2, res.indices, res.values)\n",
    "            w_new = w_new.reshape(c1, c2, width, width)\n",
    "            weights[weight_i] = w_new\n",
    "        \n",
    "    return weights    \n",
    "\n",
    "net = CAE()\n",
    "net.load_state_dict(get_weights())\n",
    "net.cuda()\n",
    "net.eval()\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "class FolderWithPath(ImageNetSubsetDataset):\n",
    "    def __init__(self, root, transform, **kwargs):\n",
    "        new_root = super(FolderWithPath, self).__init__(root, transform=transform)\n",
    "\n",
    "        classes, class_to_idx = find_classes(new_root)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        # /data/imagenet/ILSVRC/Data/CLS-LOC\n",
    "        # /home/jtang/Desktop/CAE/\n",
    "        save_path = '/home/jtang/Desktop/CAE/' + self.idx_to_class[target]\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        save_path += path[path.rindex('/'):]\n",
    "\n",
    "        if np.random.uniform() < 0.05:\n",
    "            weights = get_weights()\n",
    "            net.load_state_dict(weights)\n",
    "            net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img = trnF.to_pil_image(net(sample.unsqueeze(0).cuda()).squeeze().to('cpu').clamp(0, 1))\n",
    "\n",
    "        img.save(save_path)\n",
    "\n",
    "        return 0\n",
    "# /data/imagenet/ILSVRC/Data/CLS-LOC\n",
    "# /home/jtang/Desktop/val\n",
    "distorted_dataset = FolderWithPath(\n",
    "    root=\"/home/jtang/Desktop/val\", transform=test_transform)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(distorted_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for _ in tqdm(loader): \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actual data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_workers = 1\n",
    "worker_number = 0\n",
    "\n",
    "all_classes = [\"n01440764\", \"n01443537\", \"n01484850\", \"n01491361\", \"n01494475\", \"n01496331\", \"n01498041\", \"n01514668\", \"n01514859\", \"n01518878\", \"n01530575\", \"n01531178\", \"n01532829\", \"n01534433\", \"n01537544\", \"n01558993\", \"n01560419\", \"n01580077\", \"n01582220\", \"n01592084\", \"n01601694\", \"n01608432\", \"n01614925\", \"n01616318\", \"n01622779\", \"n01629819\", \"n01630670\", \"n01631663\", \"n01632458\", \"n01632777\", \"n01641577\", \"n01644373\", \"n01644900\", \"n01664065\", \"n01665541\", \"n01667114\", \"n01667778\", \"n01669191\", \"n01675722\", \"n01677366\", \"n01682714\", \"n01685808\", \"n01687978\", \"n01688243\", \"n01689811\", \"n01692333\", \"n01693334\", \"n01694178\", \"n01695060\", \"n01697457\", \"n01698640\", \"n01704323\", \"n01728572\", \"n01728920\", \"n01729322\", \"n01729977\", \"n01734418\", \"n01735189\", \"n01737021\", \"n01739381\", \"n01740131\", \"n01742172\", \"n01744401\", \"n01748264\", \"n01749939\", \"n01751748\", \"n01753488\", \"n01755581\", \"n01756291\", \"n01768244\", \"n01770081\", \"n01770393\", \"n01773157\", \"n01773549\", \"n01773797\", \"n01774384\", \"n01774750\", \"n01775062\", \"n01776313\", \"n01784675\", \"n01795545\", \"n01796340\", \"n01797886\", \"n01798484\", \"n01806143\", \"n01806567\", \"n01807496\", \"n01817953\", \"n01818515\", \"n01819313\", \"n01820546\", \"n01824575\", \"n01828970\", \"n01829413\", \"n01833805\", \"n01843065\", \"n01843383\", \"n01847000\", \"n01855032\", \"n01855672\", \"n01860187\", \"n01871265\", \"n01872401\", \"n01873310\", \"n01877812\", \"n01882714\", \"n01883070\", \"n01910747\", \"n01914609\", \"n01917289\", \"n01924916\", \"n01930112\", \"n01943899\", \"n01944390\", \"n01945685\", \"n01950731\", \"n01955084\", \"n01968897\", \"n01978287\", \"n01978455\", \"n01980166\", \"n01981276\", \"n01983481\", \"n01984695\", \"n01985128\", \"n01986214\", \"n01990800\", \"n02002556\", \"n02002724\", \"n02006656\", \"n02007558\", \"n02009229\", \"n02009912\", \"n02011460\", \"n02012849\", \"n02013706\", \"n02017213\", \"n02018207\", \"n02018795\", \"n02025239\", \"n02027492\", \"n02028035\", \"n02033041\", \"n02037110\", \"n02051845\", \"n02056570\", \"n02058221\", \"n02066245\", \"n02071294\", \"n02074367\", \"n02077923\", \"n02085620\", \"n02085782\", \"n02085936\", \"n02086079\", \"n02086240\", \"n02086646\", \"n02086910\", \"n02087046\", \"n02087394\", \"n02088094\", \"n02088238\", \"n02088364\", \"n02088466\", \"n02088632\", \"n02089078\", \"n02089867\", \"n02089973\", \"n02090379\", \"n02090622\", \"n02090721\", \"n02091032\", \"n02091134\", \"n02091244\", \"n02091467\", \"n02091635\", \"n02091831\", \"n02092002\", \"n02092339\", \"n02093256\", \"n02093428\", \"n02093647\", \"n02093754\", \"n02093859\", \"n02093991\", \"n02094114\", \"n02094258\", \"n02094433\", \"n02095314\", \"n02095570\", \"n02095889\", \"n02096051\", \"n02096177\", \"n02096294\", \"n02096437\", \"n02096585\", \"n02097047\", \"n02097130\", \"n02097209\", \"n02097298\", \"n02097474\", \"n02097658\", \"n02098105\", \"n02098286\", \"n02098413\", \"n02099267\", \"n02099429\", \"n02099601\", \"n02099712\", \"n02099849\", \"n02100236\", \"n02100583\", \"n02100735\", \"n02100877\", \"n02101006\", \"n02101388\", \"n02101556\", \"n02102040\", \"n02102177\", \"n02102318\", \"n02102480\", \"n02102973\", \"n02104029\", \"n02104365\", \"n02105056\", \"n02105162\", \"n02105251\", \"n02105412\", \"n02105505\", \"n02105641\", \"n02105855\", \"n02106030\", \"n02106166\", \"n02106382\", \"n02106550\", \"n02106662\", \"n02107142\", \"n02107312\", \"n02107574\", \"n02107683\", \"n02107908\", \"n02108000\", \"n02108089\", \"n02108422\", \"n02108551\", \"n02108915\", \"n02109047\", \"n02109525\", \"n02109961\", \"n02110063\", \"n02110185\", \"n02110341\", \"n02110627\", \"n02110806\", \"n02110958\", \"n02111129\", \"n02111277\", \"n02111500\", \"n02111889\", \"n02112018\", \"n02112137\", \"n02112350\", \"n02112706\", \"n02113023\", \"n02113186\", \"n02113624\", \"n02113712\", \"n02113799\", \"n02113978\", \"n02114367\", \"n02114548\", \"n02114712\", \"n02114855\", \"n02115641\", \"n02115913\", \"n02116738\", \"n02117135\", \"n02119022\", \"n02119789\", \"n02120079\", \"n02120505\", \"n02123045\", \"n02123159\", \"n02123394\", \"n02123597\", \"n02124075\", \"n02125311\", \"n02127052\", \"n02128385\", \"n02128757\", \"n02128925\", \"n02129165\", \"n02129604\", \"n02130308\", \"n02132136\", \"n02133161\", \"n02134084\", \"n02134418\", \"n02137549\", \"n02138441\", \"n02165105\", \"n02165456\", \"n02167151\", \"n02168699\", \"n02169497\", \"n02172182\", \"n02174001\", \"n02177972\", \"n02190166\", \"n02206856\", \"n02219486\", \"n02226429\", \"n02229544\", \"n02231487\", \"n02233338\", \"n02236044\", \"n02256656\", \"n02259212\", \"n02264363\", \"n02268443\", \"n02268853\", \"n02276258\", \"n02277742\", \"n02279972\", \"n02280649\", \"n02281406\", \"n02281787\", \"n02317335\", \"n02319095\", \"n02321529\", \"n02325366\", \"n02326432\", \"n02328150\", \"n02342885\", \"n02346627\", \"n02356798\", \"n02361337\", \"n02363005\", \"n02364673\", \"n02389026\", \"n02391049\", \"n02395406\", \"n02396427\", \"n02397096\", \"n02398521\", \"n02403003\", \"n02408429\", \"n02410509\", \"n02412080\", \"n02415577\", \"n02417914\", \"n02422106\", \"n02422699\", \"n02423022\", \"n02437312\", \"n02437616\", \"n02441942\", \"n02442845\", \"n02443114\", \"n02443484\", \"n02444819\", \"n02445715\", \"n02447366\", \"n02454379\", \"n02457408\", \"n02480495\", \"n02480855\", \"n02481823\", \"n02483362\", \"n02483708\", \"n02484975\", \"n02486261\", \"n02486410\", \"n02487347\", \"n02488291\", \"n02488702\", \"n02489166\", \"n02490219\", \"n02492035\", \"n02492660\", \"n02493509\", \"n02493793\", \"n02494079\", \"n02497673\", \"n02500267\", \"n02504013\", \"n02504458\", \"n02509815\", \"n02510455\", \"n02514041\", \"n02526121\", \"n02536864\", \"n02606052\", \"n02607072\", \"n02640242\", \"n02641379\", \"n02643566\", \"n02655020\", \"n02666196\", \"n02667093\", \"n02669723\", \"n02672831\", \"n02676566\", \"n02687172\", \"n02690373\", \"n02692877\", \"n02699494\", \"n02701002\", \"n02704792\", \"n02708093\", \"n02727426\", \"n02730930\", \"n02747177\", \"n02749479\", \"n02769748\", \"n02776631\", \"n02777292\", \"n02782093\", \"n02783161\", \"n02786058\", \"n02787622\", \"n02788148\", \"n02790996\", \"n02791124\", \"n02791270\", \"n02793495\", \"n02794156\", \"n02795169\", \"n02797295\", \"n02799071\", \"n02802426\", \"n02804414\", \"n02804610\", \"n02807133\", \"n02808304\", \"n02808440\", \"n02814533\", \"n02814860\", \"n02815834\", \"n02817516\", \"n02823428\", \"n02823750\", \"n02825657\", \"n02834397\", \"n02835271\", \"n02837789\", \"n02840245\", \"n02841315\", \"n02843684\", \"n02859443\", \"n02860847\", \"n02865351\", \"n02869837\", \"n02870880\", \"n02871525\", \"n02877765\", \"n02879718\", \"n02883205\", \"n02892201\", \"n02892767\", \"n02894605\", \"n02895154\", \"n02906734\", \"n02909870\", \"n02910353\", \"n02916936\", \"n02917067\", \"n02927161\", \"n02930766\", \"n02939185\", \"n02948072\", \"n02950826\", \"n02951358\", \"n02951585\", \"n02963159\", \"n02965783\", \"n02966193\", \"n02966687\", \"n02971356\", \"n02974003\", \"n02977058\", \"n02978881\", \"n02979186\", \"n02980441\", \"n02981792\", \"n02988304\", \"n02992211\", \"n02992529\", \"n02999410\", \"n03000134\", \"n03000247\", \"n03000684\", \"n03014705\", \"n03016953\", \"n03017168\", \"n03018349\", \"n03026506\", \"n03028079\", \"n03032252\", \"n03041632\", \"n03042490\", \"n03045698\", \"n03047690\", \"n03062245\", \"n03063599\", \"n03063689\", \"n03065424\", \"n03075370\", \"n03085013\", \"n03089624\", \"n03095699\", \"n03100240\", \"n03109150\", \"n03110669\", \"n03124043\", \"n03124170\", \"n03125729\", \"n03126707\", \"n03127747\", \"n03127925\", \"n03131574\", \"n03133878\", \"n03134739\", \"n03141823\", \"n03146219\", \"n03160309\", \"n03179701\", \"n03180011\", \"n03187595\", \"n03188531\", \"n03196217\", \"n03197337\", \"n03201208\", \"n03207743\", \"n03207941\", \"n03208938\", \"n03216828\", \"n03218198\", \"n03220513\", \"n03223299\", \"n03240683\", \"n03249569\", \"n03250847\", \"n03255030\", \"n03259280\", \"n03271574\", \"n03272010\", \"n03272562\", \"n03290653\", \"n03291819\", \"n03297495\", \"n03314780\", \"n03325584\", \"n03337140\", \"n03344393\", \"n03345487\", \"n03347037\", \"n03355925\", \"n03372029\", \"n03376595\", \"n03379051\", \"n03384352\", \"n03388043\", \"n03388183\", \"n03388549\", \"n03393912\", \"n03394916\", \"n03400231\", \"n03404251\", \"n03417042\", \"n03424325\", \"n03425413\", \"n03443371\", \"n03444034\", \"n03445777\", \"n03445924\", \"n03447447\", \"n03447721\", \"n03450230\", \"n03452741\", \"n03457902\", \"n03459775\", \"n03461385\", \"n03467068\", \"n03476684\", \"n03476991\", \"n03478589\", \"n03481172\", \"n03482405\", \"n03483316\", \"n03485407\", \"n03485794\", \"n03492542\", \"n03494278\", \"n03495258\", \"n03496892\", \"n03498962\", \"n03527444\", \"n03529860\", \"n03530642\", \"n03532672\", \"n03534580\", \"n03535780\", \"n03538406\", \"n03544143\", \"n03584254\", \"n03584829\", \"n03590841\", \"n03594734\", \"n03594945\", \"n03595614\", \"n03598930\", \"n03599486\", \"n03602883\", \"n03617480\", \"n03623198\", \"n03627232\", \"n03630383\", \"n03633091\", \"n03637318\", \"n03642806\", \"n03649909\", \"n03657121\", \"n03658185\", \"n03661043\", \"n03662601\", \"n03666591\", \"n03670208\", \"n03673027\", \"n03676483\", \"n03680355\", \"n03690938\", \"n03691459\", \"n03692522\", \"n03697007\", \"n03706229\", \"n03709823\", \"n03710193\", \"n03710637\", \"n03710721\", \"n03717622\", \"n03720891\", \"n03721384\", \"n03724870\", \"n03729826\", \"n03733131\", \"n03733281\", \"n03733805\", \"n03742115\", \"n03743016\", \"n03759954\", \"n03761084\", \"n03763968\", \"n03764736\", \"n03769881\", \"n03770439\", \"n03770679\", \"n03773504\", \"n03775071\", \"n03775546\", \"n03776460\", \"n03777568\", \"n03777754\", \"n03781244\", \"n03782006\", \"n03785016\", \"n03786901\", \"n03787032\", \"n03788195\", \"n03788365\", \"n03791053\", \"n03792782\", \"n03792972\", \"n03793489\", \"n03794056\", \"n03796401\", \"n03803284\", \"n03804744\", \"n03814639\", \"n03814906\", \"n03825788\", \"n03832673\", \"n03837869\", \"n03838899\", \"n03840681\", \"n03841143\", \"n03843555\", \"n03854065\", \"n03857828\", \"n03866082\", \"n03868242\", \"n03868863\", \"n03871628\", \"n03873416\", \"n03874293\", \"n03874599\", \"n03876231\", \"n03877472\", \"n03877845\", \"n03884397\", \"n03887697\", \"n03888257\", \"n03888605\", \"n03891251\", \"n03891332\", \"n03895866\", \"n03899768\", \"n03902125\", \"n03903868\", \"n03908618\", \"n03908714\", \"n03916031\", \"n03920288\", \"n03924679\", \"n03929660\", \"n03929855\", \"n03930313\", \"n03930630\", \"n03933933\", \"n03935335\", \"n03937543\", \"n03938244\", \"n03942813\", \"n03944341\", \"n03947888\", \"n03950228\", \"n03954731\", \"n03956157\", \"n03958227\", \"n03961711\", \"n03967562\", \"n03970156\", \"n03976467\", \"n03976657\", \"n03977966\", \"n03980874\", \"n03982430\", \"n03983396\", \"n03991062\", \"n03992509\", \"n03995372\", \"n03998194\", \"n04004767\", \"n04005630\", \"n04008634\", \"n04009552\", \"n04019541\", \"n04023962\", \"n04026417\", \"n04033901\", \"n04033995\", \"n04037443\", \"n04039381\", \"n04040759\", \"n04041544\", \"n04044716\", \"n04049303\", \"n04065272\", \"n04067472\", \"n04069434\", \"n04070727\", \"n04074963\", \"n04081281\", \"n04086273\", \"n04090263\", \"n04099969\", \"n04111531\", \"n04116512\", \"n04118538\", \"n04118776\", \"n04120489\", \"n04125021\", \"n04127249\", \"n04131690\", \"n04133789\", \"n04136333\", \"n04141076\", \"n04141327\", \"n04141975\", \"n04146614\", \"n04147183\", \"n04149813\", \"n04152593\", \"n04153751\", \"n04154565\", \"n04162706\", \"n04179913\", \"n04192698\", \"n04200800\", \"n04201297\", \"n04204238\", \"n04204347\", \"n04208210\", \"n04209133\", \"n04209239\", \"n04228054\", \"n04229816\", \"n04235860\", \"n04238763\", \"n04239074\", \"n04243546\", \"n04251144\", \"n04252077\", \"n04252225\", \"n04254120\", \"n04254680\", \"n04254777\", \"n04258138\", \"n04259630\", \"n04263257\", \"n04264628\", \"n04265275\", \"n04266014\", \"n04270147\", \"n04273569\", \"n04275548\", \"n04277352\", \"n04285008\", \"n04286575\", \"n04296562\", \"n04310018\", \"n04311004\", \"n04311174\", \"n04317175\", \"n04325704\", \"n04326547\", \"n04328186\", \"n04330267\", \"n04332243\", \"n04335435\", \"n04336792\", \"n04344873\", \"n04346328\", \"n04347754\", \"n04350905\", \"n04355338\", \"n04355933\", \"n04356056\", \"n04357314\", \"n04366367\", \"n04367480\", \"n04370456\", \"n04371430\", \"n04371774\", \"n04372370\", \"n04376876\", \"n04380533\", \"n04389033\", \"n04392985\", \"n04398044\", \"n04399382\", \"n04404412\", \"n04409515\", \"n04417672\", \"n04418357\", \"n04423845\", \"n04428191\", \"n04429376\", \"n04435653\", \"n04442312\", \"n04443257\", \"n04447861\", \"n04456115\", \"n04458633\", \"n04461696\", \"n04462240\", \"n04465501\", \"n04467665\", \"n04476259\", \"n04479046\", \"n04482393\", \"n04483307\", \"n04485082\", \"n04486054\", \"n04487081\", \"n04487394\", \"n04493381\", \"n04501370\", \"n04505470\", \"n04507155\", \"n04509417\", \"n04515003\", \"n04517823\", \"n04522168\", \"n04523525\", \"n04525038\", \"n04525305\", \"n04532106\", \"n04532670\", \"n04536866\", \"n04540053\", \"n04542943\", \"n04548280\", \"n04548362\", \"n04550184\", \"n04552348\", \"n04553703\", \"n04554684\", \"n04557648\", \"n04560804\", \"n04562935\", \"n04579145\", \"n04579432\", \"n04584207\", \"n04589890\", \"n04590129\", \"n04591157\", \"n04591713\", \"n04592741\", \"n04596742\", \"n04597913\", \"n04599235\", \"n04604644\", \"n04606251\", \"n04612504\", \"n04613696\", \"n06359193\", \"n06596364\", \"n06785654\", \"n06794110\", \"n06874185\", \"n07248320\", \"n07565083\", \"n07579787\", \"n07583066\", \"n07584110\", \"n07590611\", \"n07613480\", \"n07614500\", \"n07615774\", \"n07684084\", \"n07693725\", \"n07695742\", \"n07697313\", \"n07697537\", \"n07711569\", \"n07714571\", \"n07714990\", \"n07715103\", \"n07716358\", \"n07716906\", \"n07717410\", \"n07717556\", \"n07718472\", \"n07718747\", \"n07720875\", \"n07730033\", \"n07734744\", \"n07742313\", \"n07745940\", \"n07747607\", \"n07749582\", \"n07753113\", \"n07753275\", \"n07753592\", \"n07754684\", \"n07760859\", \"n07768694\", \"n07802026\", \"n07831146\", \"n07836838\", \"n07860988\", \"n07871810\", \"n07873807\", \"n07875152\", \"n07880968\", \"n07892512\", \"n07920052\", \"n07930864\", \"n07932039\", \"n09193705\", \"n09229709\", \"n09246464\", \"n09256479\", \"n09288635\", \"n09332890\", \"n09399592\", \"n09421951\", \"n09428293\", \"n09468604\", \"n09472597\", \"n09835506\", \"n10148035\", \"n10565667\", \"n11879895\", \"n11939491\", \"n12057211\", \"n12144580\", \"n12267677\", \"n12620546\", \"n12768682\", \"n12985857\", \"n12998815\", \"n13037406\", \"n13040303\", \"n13044778\", \"n13052670\", \"n13054560\", \"n13133613\", \"n15075141\"]\n",
    "all_classes.sort()\n",
    "assert len(all_classes) == 1000\n",
    "\n",
    "# Subset for this worker\n",
    "classes_chosen = np.array_split(all_classes,total_workers)[worker_number]\n",
    "\n",
    "class ImageNetSubsetDataset(torchvision.datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    Dataset class to take a specified subset of some larger dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, root, *args, **kwargs):\n",
    "        \n",
    "        print(\"Using {0} classes {1}\".format(len(classes_chosen), classes_chosen))\n",
    "\n",
    "        self.new_root = tempfile.mkdtemp()\n",
    "        for _class in classes_chosen:\n",
    "            orig_dir = os.path.join(root, _class)\n",
    "            #assert os.path.isdir(orig_dir)\n",
    "            \n",
    "            os.symlink(orig_dir, os.path.join(self.new_root, _class))\n",
    "        \n",
    "        super().__init__(self.new_root, *args, **kwargs)\n",
    "\n",
    "        return self.new_root\n",
    "    \n",
    "    def __del__(self):\n",
    "        # Clean up\n",
    "        shutil.rmtree(self.new_root)\n",
    "\n",
    "\n",
    "test_transform = trn.Compose([trn.Resize(512), trn.ToTensor()])\n",
    "\n",
    "def get_weights():\n",
    "    weight_keys = ['e_conv_1.1.weight', 'e_conv_1.1.bias', 'e_conv_2.1.weight', 'e_conv_2.1.bias', 'e_block_1.1.weight', 'e_block_1.1.bias', 'e_block_1.4.weight', 'e_block_1.4.bias', 'e_block_2.1.weight', 'e_block_2.1.bias', 'e_block_2.4.weight', 'e_block_2.4.bias', 'e_block_3.1.weight', 'e_block_3.1.bias', 'e_block_3.4.weight', 'e_block_3.4.bias', 'e_conv_3.0.weight', 'e_conv_3.0.bias', 'd_up_conv_1.0.weight', 'd_up_conv_1.0.bias', 'd_up_conv_1.3.weight', 'd_up_conv_1.3.bias', 'd_block_1.1.weight', 'd_block_1.1.bias', 'd_block_1.4.weight', 'd_block_1.4.bias', 'd_block_2.1.weight', 'd_block_2.1.bias', 'd_block_2.4.weight', 'd_block_2.4.bias', 'd_block_3.1.weight', 'd_block_3.1.bias', 'd_block_3.4.weight', 'd_block_3.4.bias', 'd_up_conv_2.0.weight', 'd_up_conv_2.0.bias', 'd_up_conv_2.3.weight', 'd_up_conv_2.3.bias', 'd_up_conv_3.0.weight', 'd_up_conv_3.0.bias', 'd_up_conv_3.3.weight', 'd_up_conv_3.3.bias']\n",
    "    key_mapping = dict([(str(int(i / 2)) + \".weight\", key) if i % 2 == 0 else (str(int(i / 2)) + \".bias\", key) for i, key in enumerate(weight_keys)])\n",
    "    NUM_LAYERS = int(len(key_mapping.values()) / 2) # 21\n",
    "    NUM_DISTORTIONS = 8\n",
    "    MODEL_PATH = \"CAE_Weights/model_final.state\"\n",
    "    OPTION_LAYER_MAPPING = {0: range(11, NUM_LAYERS - 5), 1: range(8, NUM_LAYERS - 7), 2: range(8, NUM_LAYERS - 7), 3: range(10, NUM_LAYERS - 7), 4: range(8, NUM_LAYERS - 7), 5: range(8, NUM_LAYERS - 7), 6: range(8, NUM_LAYERS - 7), 7: range(8, NUM_LAYERS - 7), 8: range(8, NUM_LAYERS - 7)}\n",
    "\n",
    "    def get_name(i, tpe):\n",
    "        return key_mapping[str(i) + \".\" + tpe]\n",
    "\n",
    "    weights = torch.load(MODEL_PATH)\n",
    "    for option in random.sample(range(NUM_DISTORTIONS), 1):\n",
    "        i = np.random.choice(OPTION_LAYER_MAPPING[option])\n",
    "        j = np.random.choice(OPTION_LAYER_MAPPING[option])\n",
    "        weight_i = get_name(i, \"weight\")\n",
    "        bias_i = get_name(i, \"bias\")\n",
    "        weight_j = get_name(j, \"weight\")\n",
    "        bias_j = get_name(j, \"weight\")\n",
    "        if option == 0:\n",
    "            weights[weight_i] = torch.flip(weights[weight_i], (0,))\n",
    "            weights[bias_i] = torch.flip(weights[bias_i], (0,))\n",
    "            weights[weight_j] = torch.flip(weights[weight_j], (0,))\n",
    "            weights[bias_j] = torch.flip(weights[bias_j], (0,))\n",
    "        elif option == 1:\n",
    "            for k in [np.random.choice(weights[weight_i].size()[0]) for _ in range(12)]:\n",
    "                weights[weight_i][k] = -weights[weight_i][k]\n",
    "                weights[bias_i][k] = -weights[bias_i][k]\n",
    "        elif option == 2:\n",
    "            for k in [np.random.choice(weights[weight_i].size()[0]) for _ in range(25)]:\n",
    "                weights[weight_i][k] = 0 * weights[weight_i][k]\n",
    "                weights[bias_i][k] = 0 * weights[bias_i][k]\n",
    "        elif option == 3:\n",
    "            for k in [np.random.choice(weights[weight_i].size()[0]) for _ in range(25)]:\n",
    "                weights[weight_i][k] = -gelu(weights[weight_i][k])\n",
    "                weights[bias_i][k] = -gelu(weights[bias_i][k])\n",
    "        elif option == 4:\n",
    "            weights[weight_i] = weights[weight_i] *\\\n",
    "            (1 + 2 * np.float32(np.random.uniform()) * (4*torch.rand_like(weights[weight_i]-1)))\n",
    "            weights[weight_j] = weights[weight_j] *\\\n",
    "            (1 + 2 * np.float32(np.random.uniform()) * (4*torch.rand_like(weights[weight_j]-1)))\n",
    "        elif option == 5: ##### begin saurav #####\n",
    "            if random.random() < 0.5:\n",
    "                mask = torch.round(torch.rand_like(weights[weight_i]))\n",
    "            else:\n",
    "                mask = torch.round(torch.rand_like(weights[weight_i])) * 2 - 1\n",
    "            weights[weight_i] *= mask\n",
    "        elif option == 6:\n",
    "            _k = random.randint(1, 3)\n",
    "            weights[weight_i] = torch.rot90(weights[weight_i], k=_k, dims=(2,3))\n",
    "        elif option == 7:\n",
    "            out_filters = weights[weight_i].shape[0]\n",
    "            to_zero = list(set([random.choice(list(range(out_filters))) for _ in range(out_filters // 5)]))\n",
    "            weights[weight_i][to_zero] = weights[weight_i][to_zero] * -1.0\n",
    "        elif option == 8:\n",
    "            # Only keep the max filter value in the conv \n",
    "            c1, c2, width = weights[weight_i].shape[0], weights[weight_i].shape[1], weights[weight_i].shape[2]\n",
    "            assert weights[weight_i].shape[2] == weights[weight_i].shape[3]\n",
    "\n",
    "            w = torch.reshape(weights[weight_i], shape=(c1, c2, width ** 2))\n",
    "            res = torch.topk(w, k=1)\n",
    "\n",
    "            w_new = torch.zeros_like(w).scatter(2, res.indices, res.values)\n",
    "            w_new = w_new.reshape(c1, c2, width, width)\n",
    "            weights[weight_i] = w_new\n",
    "        \n",
    "    return weights    \n",
    "\n",
    "net = CAE()\n",
    "net.load_state_dict(get_weights())\n",
    "net.cuda()\n",
    "net.eval()\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "class FolderWithPath(ImageNetSubsetDataset):\n",
    "    def __init__(self, root, transform, **kwargs):\n",
    "        new_root = super(FolderWithPath, self).__init__(root, transform=transform)\n",
    "\n",
    "        classes, class_to_idx = find_classes(new_root)\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (sample, target) where target is class_index of the target class.\n",
    "        \"\"\"\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        # /data/imagenet/ILSVRC/Data/CLS-LOC\n",
    "        # /home/jtang/Desktop/CAE/\n",
    "        save_path = '/data/imagenet_augment_CAE/' + self.idx_to_class[target]\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        save_path += path[path.rindex('/'):]\n",
    "\n",
    "        if np.random.uniform() < 0.05:\n",
    "            weights = get_weights()\n",
    "            net.load_state_dict(weights)\n",
    "            net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img = trnF.to_pil_image(net(sample.unsqueeze(0).cuda()).squeeze().to('cpu').clamp(0, 1))\n",
    "\n",
    "        img.save(save_path)\n",
    "\n",
    "        return 0\n",
    "# /data/imagenet/ILSVRC/Data/CLS-LOC\n",
    "# /home/jtang/Desktop/val\n",
    "distorted_dataset = FolderWithPath(\n",
    "    root=\"/data/imagenet/ILSVRC/Data/CLS-LOC/real_train\", transform=test_transform)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(distorted_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for _ in tqdm(loader): \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
